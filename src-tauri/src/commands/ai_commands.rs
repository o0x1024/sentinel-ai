//! AIç›¸å…³å‘½ä»¤æ•´åˆæ¨¡å—
//!
//! æ•´åˆäº†æ™ºèƒ½è°ƒåº¦å™¨å’ŒAIåŠ©æ‰‹çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
//! - æ™ºèƒ½æŸ¥è¯¢è°ƒåº¦
//! - æ‰§è¡Œç›‘æ§
//! - æ¶æ„ç®¡ç†
//! - Agentç»Ÿè®¡

use crate::services::{AiServiceManager, database::{Database, DatabaseService}};
// å·²åˆ é™¤ç‹¬ç«‹æ¶æ„å¼•æ“ï¼Œç°åœ¨æ‰€æœ‰èƒ½åŠ›éƒ½å†…åµŒåœ¨æ³›åŒ–çš„ ReAct å¼•æ“ä¸­
use crate::agents::traits::{ExecutionEngine, AgentTask, TaskPriority};
use sentinel_core::models::agent::{AgentTask as CoreAgentTask, TaskPriority as CoreTaskPriority, AgentExecutionResult as CoreAgentExecutionResult, SessionLog as CoreSessionLog};
use futures::StreamExt;
use sentinel_core::models::scenario_agent::{ScenarioAgentProfile, AgentEngine};

/// åˆ›å»ºAIåŠ©æ‰‹ä¼šè¯è®°å½•
async fn create_ai_assistant_session(
    db_service: &Arc<DatabaseService>,
    execution_id: &str,
    agent_name: &str,
    task_description: &str,
) -> Result<(), String> {
    use crate::services::database::Database;

    // åˆ›å»ºtask_id
    let task_id = format!("{}_task", execution_id);

    // å…ˆåˆ›å»ºagent_taskè®°å½•ï¼ˆå› ä¸ºagent_sessionsè¡¨æœ‰å¤–é”®çº¦æŸï¼‰
    let agent_task = crate::agents::traits::AgentTask {
        id: task_id.clone(),
        user_id: "ai_assistant".to_string(),
        description: task_description.to_string(),
        priority: crate::agents::traits::TaskPriority::Normal,
        target: None,
        parameters: std::collections::HashMap::new(),
        timeout: Some(300),
    };

    let db_task = CoreAgentTask {
        id: agent_task.id.clone(),
        description: agent_task.description.clone(),
        target: agent_task.target.clone(),
        parameters: agent_task.parameters.clone(),
        user_id: agent_task.user_id.clone(),
        priority: match agent_task.priority {
            TaskPriority::Low => CoreTaskPriority::Low,
            TaskPriority::Normal => CoreTaskPriority::Normal,
            TaskPriority::High => CoreTaskPriority::High,
            TaskPriority::Critical => CoreTaskPriority::Critical,
        },
        timeout: agent_task.timeout,
    };

    db_service.create_agent_task(&db_task).await
        .map_err(|e| format!("Failed to create agent task: {}", e))?;

    // ç„¶ååˆ›å»ºagent_sessionè®°å½•
    db_service.create_agent_session(execution_id, &task_id, agent_name).await
        .map_err(|e| format!("Failed to create agent session: {}", e))?;

    Ok(())
}

/// ä¿å­˜AIåŠ©æ‰‹æ‰§è¡Œè®°å½•åˆ°æ•°æ®åº“
async fn save_ai_assistant_execution(
    db_service: &Arc<DatabaseService>,
    execution_id: &str,
    _task_name: &str,
    architecture: &str,
    success: bool,
    error: Option<&str>,
    result: Option<&str>,
    started_at: u64,
    completed_at: u64,
    duration_ms: u64,
) -> Result<(), String> {
    use crate::services::database::Database;
use sentinel_core::models::workflow::WorkflowStepDetail;

    // ä¿å­˜æ‰§è¡Œæ­¥éª¤åˆ° agent_execution_steps è¡¨
    let step_detail = WorkflowStepDetail {
        step_id: "step_1".to_string(),
        step_name: format!("AI Assistant Task ({})", architecture),
        status: if success { "Completed".to_string() } else { "Failed".to_string() },
        started_at: Some(started_at.to_string()),
        completed_at: Some(completed_at.to_string()),
        duration_ms,
        result_data: result.map(|r| serde_json::json!(r)),
        error: error.map(|e| e.to_string()),
        retry_count: 0,
        dependencies: vec![],
        tool_result: None,
    };

    db_service.save_agent_execution_step(execution_id, &step_detail).await
        .map_err(|e| format!("Failed to save execution step: {}", e))?;

    // æ›´æ–°sessionçŠ¶æ€
    let status_str = if success { "Completed" } else { "Failed" };
    if let Err(e) = db_service.update_agent_session_status(execution_id, status_str).await {
        log::warn!("Failed to update agent session status: {}", e);
    }

    Ok(())
}



use tauri::{AppHandle, State, Emitter, Manager};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::collections::HashMap;
use uuid::Uuid;
use anyhow::Result;
use log::{info, warn};

/// å‘½ä»¤å“åº”åŒ…è£…å™¨
#[derive(Debug, Serialize)]
pub struct CommandResponse<T> {
    pub success: bool,
    pub data: Option<T>,
    pub error: Option<String>,
    pub timestamp: u64,
    pub request_id: String,
}

impl<T> CommandResponse<T> {
    pub fn success(data: T) -> Self {
        Self {
            success: true,
            data: Some(data),
            error: None,
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            request_id: Uuid::new_v4().to_string(),
        }
    }

    pub fn error(message: String) -> Self {
        Self {
            success: false,
            data: None,
            error: Some(message),
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            request_id: Uuid::new_v4().to_string(),
        }
    }
}

// ===== æ™ºèƒ½è°ƒåº¦å™¨ç›¸å…³ç»“æ„ä½“ =====


#[derive(Debug, Serialize)]
pub struct IntelligentQueryResponse {
    pub request_id: String,
    pub execution_id: String,
    pub selected_architecture: String,
    pub task_type: String,
    pub complexity: String,
    pub reasoning: String,
    pub confidence: f32,
    pub estimated_duration: Option<f64>,
    pub workflow_status: String,
    pub started_at: String,
}

#[derive(Debug, Deserialize)]
pub struct ExecutionStatusRequest {
    pub id: String,
    pub id_type: String,
}

#[derive(Debug, Serialize)]
pub struct ExecutionStatusResponse {
    pub execution_id: String,
    pub request_id: String,
    pub status: String,
    pub progress: f32,
    pub current_step: Option<String>,
    pub completed_steps: u32,
    pub total_steps: u32,
    pub started_at: String,
    pub completed_at: Option<String>,
    pub result: Option<serde_json::Value>,
    pub error: Option<String>,
}

#[derive(Debug, Deserialize)]
pub struct ExecutionHistoryRequest {
    pub user_id: Option<String>,
    pub architecture: Option<String>,
    pub status: Option<String>,
    pub page: Option<u32>,
    pub page_size: Option<u32>,
    pub start_time: Option<String>,
    pub end_time: Option<String>,
}

#[derive(Debug, Serialize)]
pub struct ExecutionHistoryResponse {
    pub records: Vec<ExecutionHistoryItem>,
    pub total: u32,
    pub page: u32,
    pub page_size: u32,
    pub total_pages: u32,
}

#[derive(Debug, Serialize)]
pub struct ExecutionHistoryItem {
    pub request_id: String,
    pub execution_id: String,
    pub user_input: String,
    pub architecture: String,
    pub task_type: String,
    pub complexity: String,
    pub status: String,
    pub execution_time: Option<f64>,
    pub success_rate: Option<f32>,
    pub started_at: String,
    pub completed_at: Option<String>,
}

#[derive(Debug, Serialize)]
pub struct DispatcherStatistics {
    pub total_requests: f64,
    pub successful_requests: f64,
    pub failed_requests: f64,
    pub average_execution_time: f64,
    pub architecture_usage: HashMap<String, f64>,
    pub uptime_seconds: f64,
}

// ===== AIåŠ©æ‰‹ç›¸å…³ç»“æ„ä½“ =====

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DispatchQueryRequest {
    pub query: String,
    pub architecture: String,
    pub agent_id: Option<String>,
    pub options: Option<HashMap<String, serde_json::Value>>,
    pub conversation_id: Option<String>,
    pub message_id: Option<String>,
}




#[derive(Debug, Serialize, Deserialize)]
pub struct DispatchResult {
    pub execution_id: String,
    pub initial_response: String,
    pub execution_plan: Option<ExecutionPlanView>,
    pub estimated_duration: u64,
    pub selected_architecture: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ExecutionPlanView {
    pub id: String,
    pub name: String,
    pub description: String,
    pub steps: Vec<ExecutionStepView>,
    pub estimated_duration: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ExecutionStepView {
    pub id: String,
    pub name: String,
    pub description: String,
    pub status: String,
    pub estimated_duration: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AIAssistantSettings {
    #[serde(default)]
    pub auto_execute: bool,
    #[serde(default = "default_notification_enabled")]
    pub notification_enabled: bool,
}

fn default_notification_enabled() -> bool {
    true
}

// ===== åœºæ™¯ Agent Profileï¼ˆæœ€å°å¯ç”¨ç‰ˆæœ¬ï¼‰=====


// Expose tools catalog for agent configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SimpleToolInfo {
    pub name: String,
    pub title: Option<String>,
    pub category: Option<String>,
    pub description: Option<String>,
    pub available: bool,
    pub source: Option<String>,
    pub group: Option<String>,
}

#[tauri::command]
pub async fn list_unified_tools(
    tool_system: State<'_, Arc<crate::tools::ToolSystem>>,
) -> Result<Vec<SimpleToolInfo>, String> {
    let tools = tool_system.list_tools().await;
    let list = tools
        .into_iter()
        .map(|t| SimpleToolInfo {
            name: t.name,
            title: None, // ToolMetadata æ²¡æœ‰é€šç”¨ title å­—æ®µï¼Œè¿™é‡Œä¸ºç©º
            category: Some(t.category.to_string()),
            description: if t.description.is_empty() { None } else { Some(t.description) },
            available: t.available,
            source: {
                // ä¼˜å…ˆç”¨metadata.tagsåˆ¤æ–­mcpï¼Œå¦åˆ™fallback
                let tag_has_mcp = t.metadata.tags.iter().any(|x| x == "mcp");
                Some(if tag_has_mcp { "mcp".to_string() } else { "builtin".to_string() })
            },
            group: t.metadata.tags.iter()
                .find_map(|tag| tag.strip_prefix("connection:").map(|s| s.to_string())),
        })
        .collect();
    Ok(list)
}

// åˆ†ç»„è¿”å›ï¼šå†…ç½®å·¥å…· + MCPæŒ‰è¿æ¥åˆ†ç»„
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct McpToolGroup { pub connection: String, pub tools: Vec<SimpleToolInfo> }

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GroupedToolsResponse { pub builtin: Vec<SimpleToolInfo>, pub mcp: Vec<McpToolGroup> }

#[tauri::command]
pub async fn list_unified_tools_grouped(
    tool_system: State<'_, Arc<crate::tools::ToolSystem>>,
) -> Result<GroupedToolsResponse, String> {
    let tools = tool_system.list_tools().await;
    let mut builtin: Vec<SimpleToolInfo> = Vec::new();
    let mut groups: std::collections::HashMap<String, Vec<SimpleToolInfo>> = std::collections::HashMap::new();

    for t in tools.into_iter() {
        // è·³è¿‡æ’ä»¶å·¥å…·ï¼ˆåç§°ä»¥ plugin:: å¼€å¤´æˆ–åŒ…å« plugin æ ‡ç­¾ï¼‰
        // æ’ä»¶å·¥å…·åº”è¯¥é€šè¿‡ list_plugins æ¥å£å•ç‹¬ç®¡ç†
        let is_plugin = t.name.starts_with("plugin::") || t.metadata.tags.iter().any(|x| x == "plugin");
        if is_plugin {
            continue;
        }

        let is_mcp = t.metadata.tags.iter().any(|x| x == "mcp");
        let group = t.metadata.tags.iter()
            .find_map(|tag| tag.strip_prefix("connection:").map(|s| s.to_string()));
        let item = SimpleToolInfo {
            name: t.name,
            title: None,
            category: Some(t.category.to_string()),
            description: if t.description.is_empty() { None } else { Some(t.description) },
            available: t.available,
            source: Some(if is_mcp { "mcp".to_string() } else { "builtin".to_string() }),
            group: group.clone(),
        };

        if is_mcp {
            let key = group.unwrap_or_else(|| "unknown".to_string());
            groups.entry(key).or_default().push(item);
        } else {
            builtin.push(item);
        }
    }

    let mut mcp: Vec<McpToolGroup> = groups.into_iter()
        .map(|(k, v)| McpToolGroup { connection: k, tools: v })
        .collect();
    // ç¨³å®šæ’åºè¿æ¥å
    mcp.sort_by(|a, b| a.connection.cmp(&b.connection));

    Ok(GroupedToolsResponse { builtin, mcp })
}

#[tauri::command]
pub async fn list_scenario_agents(
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<Vec<ScenarioAgentProfile>, String> {
    db_service
        .list_scenario_agents()
        .await
        .map_err(|e| format!("Failed to load scenario agents: {}", e))
}

#[tauri::command]
pub async fn save_scenario_agent(
    profile: ScenarioAgentProfile,
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<(), String> {
    db_service
        .upsert_scenario_agent(&profile)
        .await
        .map_err(|e| format!("Failed to save scenario agent: {}", e))
}

#[tauri::command]
pub async fn delete_scenario_agent(
    id: String,
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<(), String> {
    db_service
        .delete_scenario_agent(&id)
        .await
        .map_err(|e| format!("Failed to delete scenario agent: {}", e))
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScenarioTaskDispatchRequest {
    pub agent_id: String,
    pub query: String,
    pub options: Option<HashMap<String, serde_json::Value>>,
    pub conversation_id: Option<String>,
    pub message_id: Option<String>,
}

#[tauri::command]
pub async fn dispatch_scenario_task(
    request: ScenarioTaskDispatchRequest,
    db_service: State<'_, Arc<DatabaseService>>,
    ai_service_manager: State<'_, Arc<AiServiceManager>>,
    execution_manager: State<'_, Arc<crate::managers::ExecutionManager>>,
    app_handle: AppHandle,
) -> Result<DispatchResult, String> {
    // è¯»å– Agent Profile
    let agents = list_scenario_agents(db_service.clone()).await?;
    let Some(profile) = agents.into_iter().find(|p| p.id == request.agent_id && p.enabled) else {
        return Err(format!("Scenario agent not found or disabled: {}", request.agent_id));
    };

    // é€‰æ¶æ„ï¼ˆç»Ÿä¸€ä½¿ç”¨ ReAct æ³›åŒ–å¼•æ“ï¼‰
    // æ‰€æœ‰æ¶æ„ç±»å‹éƒ½æ˜ å°„åˆ° reactï¼Œä»»åŠ¡ç‰¹æ€§é€šè¿‡ Prompt é…ç½®
    let architecture = match profile.engine {
        AgentEngine::React | AgentEngine::Travel | AgentEngine::PlanExecute | 
        AgentEngine::Rewoo | AgentEngine::LlmCompiler | AgentEngine::Auto => "react",
    }.to_string();

    let mut options = request.options.unwrap_or_default();
    options.insert("agent_id".to_string(), serde_json::Value::String(request.agent_id.clone()));

    // ä» options ä¸­æå– conversation_id å’Œ message_idï¼ˆå‘åå…¼å®¹å‰ç«¯æŠŠå®ƒä»¬æ”¾åœ¨ options é‡Œçš„æƒ…å†µï¼‰
    let conversation_id = request.conversation_id.clone()
        .or_else(|| options.get("conversation_id").and_then(|v| v.as_str()).map(|s| s.to_string()));
    let message_id = request.message_id.clone()
        .or_else(|| options.get("message_id").and_then(|v| v.as_str()).map(|s| s.to_string()));

    if let Some(conv_id) = conversation_id.as_ref() {
        if !conv_id.is_empty() {
            let user_msg = sentinel_core::models::database::AiMessage {
                id: uuid::Uuid::new_v4().to_string(),
                conversation_id: conv_id.clone(),
                role: "user".to_string(),
                content: request.query.clone(),
                metadata: None,
                token_count: Some(request.query.len() as i32),
                cost: None,
                tool_calls: None,
                attachments: None,
                timestamp: chrono::Utc::now(),
                architecture_type: None,
                architecture_meta: None,
                structured_data: None,
            };
            let _ = db_service.create_message(&user_msg).await;
        }
    }

    // è·å–å½“å‰è§’è‰²æç¤ºè¯å¹¶æ·»åŠ åˆ°optionsä¸­
    if let Ok(Some(current_role)) = db_service.get_current_ai_role().await {
        if !current_role.prompt.trim().is_empty() {
            options.insert("role_prompt".to_string(), serde_json::Value::String(current_role.prompt));
            tracing::info!("Added role prompt from: {}", current_role.title);
        }
    }

    // é€ä¼ å·²ç»‘å®šçš„æç¤ºè¯æ¨¡æ¿IDï¼Œä¾›å¼•æ“æˆ–æ‰§è¡Œå±‚ä½¿ç”¨
    if let Some(pids) = &profile.prompt_ids {
        options.insert("prompt_ids".to_string(), serde_json::json!({
            "system": pids.system,
            "planner": pids.planner,
            "executor": pids.executor,
            "replanner": pids.replanner,
            "evaluator": pids.evaluator,
        }));
    }
    // é€ä¼ ç»Ÿä¸€æç¤ºè¯ç³»ç»Ÿç­–ç•¥ã€åˆ†ç»„ã€æ–‡æœ¬è¦†ç›–åŠç‰ˆæœ¬å›ºå®š
    if let Some(strategy) = &profile.prompt_strategy {
        options.insert("prompt_strategy".to_string(), serde_json::Value::String(strategy.clone()));
    }
    if let Some(gid) = profile.group_id {
        options.insert("group_id".to_string(), serde_json::json!(gid));
    }
    {
        let prompts = &profile.prompts;
        options.insert("prompts".to_string(), serde_json::json!({
            "system": prompts.system,
            "planner": prompts.planner,
            "executor": prompts.executor,
            "replanner": prompts.replanner,
            "evaluator": prompts.evaluator,
        }));
    }
    if let Some(pinned) = &profile.pinned_versions {
        options.insert("pinned_versions".to_string(), serde_json::to_value(pinned).unwrap_or_else(|_| serde_json::json!({})));
    }

    // å·¥å…·ç™½åå•/é»‘åå•ç­–ç•¥ï¼ˆç”¨äºæ‰§è¡ŒæœŸè¿‡æ»¤ï¼‰
    // è¦æ±‚ï¼šSystem prompt ä¸­çš„å·¥å…·æ¸…å•åº”ä¸¥æ ¼ä¾æ® AgentManager.vue ä¸­â€œå¯ç”¨+å·²é€‰â€çš„é›†åˆã€‚
    // è¯­ä¹‰ï¼š
    // - è‹¥å‰ç«¯é…ç½®å­˜åœ¨ï¼ˆprofile.tools æœ‰å€¼ï¼‰ï¼šæŒ‰ allow/deny é€ä¼ ï¼›
    // - è‹¥å‰ç«¯æœªé…ç½®ï¼ˆprofile.tools ä¸º Noneï¼‰ï¼šä¹Ÿè¦æ˜¾å¼ä¼ å…¥ç©ºç™½åå•ï¼Œè¡¨ç¤ºâ€œæœªé€‰æ‹©ä»»ä½•å·¥å…· â‡’ ç¦ç”¨æ‰€æœ‰å·¥å…·â€ã€‚
    //   è¿™æ · ReAct/Planner åœ¨æ„å»ºå·¥å…·æ¸…å•æ—¶ä¸ä¼šé€€å›åˆ°â€œå…è®¸æ‰€æœ‰â€ã€‚
    {
        let tool_policy = &profile.tools;
        log::info!("Agent tools policy - allow: {:?}, deny: {:?}", tool_policy.allow, tool_policy.deny);
        options.insert(
            "tools_allow".to_string(),
            serde_json::json!(tool_policy.allow.clone())
        );
        if let Some(deny) = &tool_policy.deny {
            options.insert("tools_deny".to_string(), serde_json::json!(deny.clone()));
        }
    }

    // æ‰§è¡Œç­–ç•¥ï¼ˆè¶…æ—¶/é‡è¯•/ä¸¥æ ¼æ¨¡å¼/å¹¶å‘ï¼‰
    {
        let exec = &profile.execution;
        if let Some(timeout) = exec.timeout_sec {
            options.insert("execution_timeout_sec".to_string(), serde_json::json!(timeout));
        }
        let retry = &exec.retry;
        options.insert("execution_retry_max".to_string(), serde_json::json!(retry.max_retries));
        options.insert("execution_retry_backoff".to_string(), serde_json::json!(retry.backoff.clone()));
        if let Some(iv) = retry.interval_ms { options.insert("execution_retry_interval_ms".to_string(), serde_json::json!(iv)); }
        if let Some(conc) = exec.concurrency { options.insert("execution_concurrency".to_string(), serde_json::json!(conc)); }
        if let Some(strict) = exec.strict_mode { options.insert("execution_strict_mode".to_string(), serde_json::json!(strict)); }
    }

    // LLMé…ç½®ï¼ˆç”¨äºè¦†ç›–é˜¶æ®µé»˜è®¤æ¨¡å‹ï¼‰
    // ç›´æ¥ä¼ é€’å®Œæ•´ç»“æ„ï¼Œä¾¿äºåç»­è§£æ
    options.insert(
        "llm".to_string(),
        serde_json::json!({
            "default": {
                "provider": profile.llm.default.provider,
                "model": profile.llm.default.model,
                "temperature": profile.llm.default.temperature,
                "max_tokens": profile.llm.default.max_tokens,
            }
        })
    );

    // ä»¥ä¸‹æ˜¯åŸ dispatch_intelligent_query çš„é€»è¾‘
    // æå–ä»»åŠ¡æ¨¡å¼æ ‡è¯†å’Œç›¸å…³ä¿¡æ¯
    let is_task_mode = options.get("task_mode")
        .and_then(|v| v.as_bool())
        .unwrap_or(false);

    // conversation_id å’Œ message_id å·²ç»åœ¨ä¸Šé¢ä» request æˆ– options ä¸­æå–
    // è¿™é‡Œä¸éœ€è¦å†æ¬¡æå–ï¼Œç›´æ¥ä½¿ç”¨ä¹‹å‰çš„å˜é‡

    let execution_id = options.get("execution_id")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string())
        .unwrap_or_else(|| uuid::Uuid::new_v4().to_string());

    // åˆ›å»ºagent_sessionè®°å½•ç”¨äºç»Ÿä¸€çš„å·¥ä½œæµç›‘æ§
    if let Err(e) = create_ai_assistant_session(
        &db_service,
        &execution_id,
        &profile.name,
        &request.query,
    ).await {
        log::warn!("Failed to create AI assistant session: {}", e);
    }

    // æå–ä¼šè¯IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
    if let Some(ref conv_id) = conversation_id {
        if !conv_id.is_empty() {
            info!("Processing request for conversation: {}", conv_id);
        }
    }

    // å¦‚æœæ˜¯ä»»åŠ¡æ¨¡å¼ä¸”æ¶æ„ä¸º"auto"ï¼Œè¿›è¡Œæ™ºèƒ½é€‰æ‹©
    let selected_architecture = if is_task_mode && architecture == "auto" {
        let auto_selected = select_best_architecture(&request.query).await
            .map_err(|e| format!("Failed to select architecture: {}", e))?;

        info!("Auto-selected architecture: {} for query: {}", auto_selected, request.query);

        auto_selected
    } else {
        architecture.clone()
    };

    // åˆ›å»º DispatchQueryRequest
    let dispatch_req = DispatchQueryRequest {
        query: request.query,
        architecture: selected_architecture.clone(),
        agent_id: Some(profile.id),
        options: Some(options),
        conversation_id: conversation_id.clone(),
        message_id: message_id.clone(),
    };

    let app_clone = app_handle.clone();
    // æ‰€æœ‰æ¶æ„ç»Ÿä¸€ä½¿ç”¨æ³›åŒ–çš„ ReAct å¼•æ“
    // PlanExecuteã€ReWOOã€LLMCompilerã€Travel èƒ½åŠ›å·²å…¨éƒ¨å†…åµŒ
    let result = dispatch_with_react(
        execution_id.clone(),
        dispatch_req,
        (*ai_service_manager).clone(),
        (*db_service).clone(),
        (*execution_manager).clone(),
        app_clone.clone(),
    ).await;

    // ReAct å¼•æ“åœ¨ dispatch_with_react ä¸­ç›´æ¥å¼‚æ­¥æ‰§è¡Œï¼Œä¸éœ€è¦é¢å¤–çš„æ‰§è¡Œè§¦å‘
    // ç›´æ¥è¿”å›è°ƒåº¦ç»“æœ
    result
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AgentStatistics {
    pub active_count: u32,
    pub total_tasks: u32,
    pub successful_tasks: u32,
    pub failed_tasks: u32,
    pub average_execution_time: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CustomAgent {
    pub id: String,
    pub name: String,
    pub description: String,
    #[serde(rename = "type")]
    pub r#type: String,
    pub status: String,
    pub tasks_completed: u32,
}



/// åœæ­¢æ‰§è¡Œ
#[tauri::command(rename_all = "snake_case")]
pub async fn stop_execution(
    execution_id: String,
    app: AppHandle,
) -> Result<(), String> {
    info!("ğŸ›‘ Stopping execution: {}", execution_id);

    // 1. âœ… å–æ¶ˆCancellationTokenï¼ˆå¯¹ReActæ¶æ„æœ‰æ•ˆï¼‰
    use crate::managers::cancellation_manager;
    let cancelled_by_token = cancellation_manager::cancel_execution(&execution_id).await;
    if cancelled_by_token {
        log::info!("âœ… Cancelled execution via CancellationToken: {}", execution_id);
    }

    // 2. å°è¯•åœæ­¢æ‰§è¡Œç®¡ç†å™¨ä¸­çš„ä»»åŠ¡ï¼ˆå¯¹Plan-Execute/LLMCompileræœ‰æ•ˆï¼‰
    let execution_manager = app.state::<Arc<crate::managers::ExecutionManager>>();
    let manager = execution_manager.inner().clone();
    if let Err(e) = manager.stop_execution(&execution_id).await {
        log::warn!("Failed to stop execution via ExecutionManager {}: {}", execution_id, e);
    } else {
        log::info!("âœ… Stopped execution via ExecutionManager: {}", execution_id);
    }

    // 3. å¦‚æœexecution_idçœ‹èµ·æ¥åƒä¼šè¯IDï¼Œä¹Ÿå°è¯•å–æ¶ˆå¯¹åº”çš„æµ
    // è¿™æ ·å¯ä»¥å¤„ç†ç”¨ä¼šè¯IDè°ƒç”¨stopçš„æƒ…å†µ
    if execution_id.starts_with("conv_") || execution_id.len() == 36 {
        // å¯èƒ½æ˜¯ä¼šè¯IDæˆ–UUIDæ ¼å¼
        use crate::commands::ai::cancel_conversation_stream;
        cancel_conversation_stream(&execution_id);
        log::info!("âœ… Cancelled stream for conversation: {}", execution_id);
    }

    // 4. å‘é€åœæ­¢äº‹ä»¶ï¼ˆç»Ÿä¸€äº‹ä»¶åç§°ï¼‰
    if let Err(e) = app.emit("execution_stopped", serde_json::json!({
        "execution_id": execution_id,
        "message": "Execution stopped by user"
    })) {
        log::warn!("Failed to emit execution_stopped event: {}", e);
    }

    log::info!("âœ… Stop execution completed: {}", execution_id);

    info!("Execution stop completed: {}", execution_id);
    Ok(())
}

/// è·å–AIåŠ©æ‰‹è®¾ç½®
#[tauri::command]
pub async fn get_ai_assistant_settings(
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<AIAssistantSettings, String> {
    // ä»æ•°æ®åº“åŠ è½½è®¾ç½®ï¼Œä½¿ç”¨ä¸“é—¨çš„keyå­˜å‚¨AIAssistantSettings
    match db_service.get_config("ai_assistant", "assistant_settings").await {
        Ok(Some(json_str)) => {
            serde_json::from_str::<AIAssistantSettings>(&json_str)
                .map_err(|e| format!("Failed to parse AI assistant settings: {}", e))
        }
        Ok(None) => {
            // è¿”å›é»˜è®¤è®¾ç½®
            let default_settings = AIAssistantSettings {
                auto_execute: false,
                notification_enabled: true,
            };
            info!("Using default AI assistant settings");
            Ok(default_settings)
        }
        Err(e) => Err(format!("Failed to load AI assistant settings: {}", e)),
    }
}

/// ä¿å­˜AIåŠ©æ‰‹è®¾ç½®
#[tauri::command]
pub async fn save_ai_assistant_settings(
    settings: AIAssistantSettings,
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<(), String> {
    info!("Saving AI assistant settings: {:?}", settings);
    let json = serde_json::to_string(&settings)
        .map_err(|e| format!("Failed to serialize settings: {}", e))?;
    db_service
        .set_config(
            "ai_assistant",
            "assistant_settings",
            &json,
            None,
        )
        .await
        .map_err(|e| format!("Failed to save AI assistant settings: {}", e))
}

/// è·å–Agentç»Ÿè®¡ä¿¡æ¯
#[tauri::command]
pub async fn get_agent_statistics(
    manager: State<'_, crate::commands::agent_commands::GlobalAgentManager>,
) -> Result<AgentStatistics, String> {
    let manager_guard = manager.read().await;

    let agent_manager = match manager_guard.as_ref() {
        Some(manager) => manager,
        None => {
            // Agentç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¿”å›é»˜è®¤å€¼
            return Ok(AgentStatistics {
                active_count: 0,
                total_tasks: 0,
                successful_tasks: 0,
                failed_tasks: 0,
                average_execution_time: 0.0,
            });
        }
    };

    // ä»Agentç®¡ç†å™¨è·å–çœŸå®ç»Ÿè®¡æ•°æ®
    let stats = agent_manager.get_statistics().await;
    let sessions = agent_manager.get_all_sessions().await;

    // ç»Ÿè®¡æ´»è·ƒä¼šè¯æ•°
    let active_count = sessions.iter().filter(|(_, info)| {
        matches!(
            info.status,
            crate::agents::traits::AgentSessionStatus::Planning |
            crate::agents::traits::AgentSessionStatus::Executing
        )
    }).count();

    Ok(AgentStatistics {
        active_count: active_count as u32,
        total_tasks: stats.total_tasks as u32,
        successful_tasks: stats.successful_tasks as u32,
        failed_tasks: stats.failed_tasks as u32,
        average_execution_time: stats.average_execution_time_ms / 1000.0, // è½¬æ¢ä¸ºç§’
    })
}

/// è·å–å¯ç”¨æ¶æ„åˆ—è¡¨
#[tauri::command]
pub async fn get_available_architectures() -> Result<Vec<serde_json::Value>, String> {
    Ok(vec![
        serde_json::json!({
            "id": "auto",
            "name": "Auto",
            "description": "è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¶æ„",
            "suitable_for": ["æ‰€æœ‰ä»»åŠ¡"],
            "performance": "è‡ªåŠ¨",
            "status": "stable"
        }),
        serde_json::json!({
            "id": "plan-execute",
            "name": "Plan-and-Execute",
            "description": "åŸºäºè§„åˆ’å’Œæ‰§è¡Œçš„æ™ºèƒ½Agentæ¶æ„",
            "suitable_for": ["å¤æ‚ä»»åŠ¡", "å¤šæ­¥éª¤æµç¨‹", "éœ€è¦é‡è§„åˆ’çš„ä»»åŠ¡"],
            "performance": "ç¨³å®š",
            "status": "stable"
        }),
        serde_json::json!({
            "id": "rewoo",
            "name": "ReWOO",
            "description": "æ¨ç†è€Œéè§‚å¯Ÿçš„å·¥ä½œæµæ¶æ„",
            "suitable_for": ["æ¨ç†å¯†é›†å‹ä»»åŠ¡", "åˆ†æç±»ä»»åŠ¡"],
            "performance": "é«˜æ•ˆ",
            "status": "beta"
        }),
        serde_json::json!({
            "id": "llm-compiler",
            "name": "LLMCompiler",
            "description": "å¹¶è¡Œæ‰§è¡Œå¼•æ“",
            "suitable_for": ["å¹¶è¡Œä»»åŠ¡", "ç‹¬ç«‹çš„å¤šä¸ªå­ä»»åŠ¡"],
            "performance": "å¿«é€Ÿ",
            "status": "experimental"
        }),

    ])
}

/// è·å–è‡ªå®šä¹‰Agentåˆ—è¡¨
#[tauri::command]
pub async fn get_ai_assistant_agents(
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<Vec<CustomAgent>, String> {
    match db_service.get_config("ai_assistant", "custom_agents").await {
        Ok(Some(json_str)) => serde_json::from_str::<Vec<CustomAgent>>(&json_str)
            .map_err(|e| format!("Failed to parse custom agents: {}", e)),
        Ok(None) => Ok(vec![]),
        Err(e) => Err(format!("Failed to load custom agents: {}", e)),
    }
}

/// ä¿å­˜è‡ªå®šä¹‰Agentåˆ—è¡¨
#[tauri::command]
pub async fn save_ai_assistant_agents(
    agents: Vec<CustomAgent>,
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<(), String> {
    let json = serde_json::to_string(&agents)
        .map_err(|e| format!("Failed to serialize custom agents: {}", e))?;
    db_service
        .set_config(
            "ai_assistant",
            "custom_agents",
            &json,
            None,
        )
        .await
        .map_err(|e| format!("Failed to save custom agents: {}", e))
}

/// è·å–æ¶æ„å¯ç”¨åå¥½ï¼ˆè¿”å›å¯ç”¨çš„æ¶æ„IDåˆ—è¡¨ï¼‰
#[tauri::command]
pub async fn get_ai_architecture_prefs(
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<Vec<String>, String> {
    match db_service.get_config("ai_assistant", "enabled_architectures").await {
        Ok(Some(json_str)) => serde_json::from_str::<Vec<String>>(&json_str)
            .map_err(|e| format!("Failed to parse architecture prefs: {}", e)),
        Ok(None) => Ok(vec![
            "plan-execute".to_string(),
            "rewoo".to_string(),
            "llm-compiler".to_string(),
        ]),
        Err(e) => Err(format!("Failed to load architecture prefs: {}", e)),
    }
}

/// ä¿å­˜æ¶æ„å¯ç”¨åå¥½
#[tauri::command]
pub async fn save_ai_architecture_prefs(
    enabled_architectures: Vec<String>,
    db_service: State<'_, Arc<DatabaseService>>,
) -> Result<(), String> {
    let json = serde_json::to_string(&enabled_architectures)
        .map_err(|e| format!("Failed to serialize architecture prefs: {}", e))?;
    db_service
        .set_config(
            "ai_assistant",
            "enabled_architectures",
            &json,
            None,
        )
        .await
        .map_err(|e| format!("Failed to save architecture prefs: {}", e))
}

// ===== è¾…åŠ©å‡½æ•°å’Œç»“æ„ä½“ =====

#[derive(Debug, Deserialize)]
pub struct TaskSubmissionRequest {
    pub user_input: String,
    pub user_id: String,
    pub priority: Option<String>,
    pub estimated_duration: Option<f64>,
    pub metadata: Option<HashMap<String, serde_json::Value>>,
}

#[derive(Debug, Deserialize)]
pub struct NodeRegistrationRequest {
    pub name: String,
    pub capacity: NodeCapacityRequest,
}

#[derive(Debug, Deserialize)]
pub struct NodeCapacityRequest {
    pub cpu_cores: u32,
    pub memory_gb: u32,
    pub network_mbps: f32,
    pub storage_gb: u32,
    pub max_concurrent_tasks: u32,
}



/// æ™ºèƒ½é€‰æ‹©æœ€ä½³æ¶æ„
async fn select_best_architecture(_user_input: &str) -> Result<String, String> {
    // æ‰€æœ‰ä»»åŠ¡ç»Ÿä¸€ä½¿ç”¨æ³›åŒ–çš„ ReAct å¼•æ“
    // ä»»åŠ¡ç‰¹æ€§é€šè¿‡ Prompt é…ç½®å®ç°ï¼Œè€Œéä»£ç é€‰æ‹©ä¸åŒæ¶æ„
    Ok("react".to_string())
}



async fn dispatch_with_react(
    execution_id: String,
    request: DispatchQueryRequest,
    ai_service_manager: Arc<AiServiceManager>,
    db_service: Arc<DatabaseService>,
    _execution_manager: Arc<crate::managers::ExecutionManager>,
    app: AppHandle,
) -> Result<DispatchResult, String> {
    use crate::engines::react::{ReactEngine, ReactConfig};
    use std::collections::HashMap;
    use crate::agents::traits::{AgentTask, TaskPriority};
    use crate::managers::cancellation_manager;

    info!("Creating ReAct dispatch for: {}", request.query);

    // âœ… æ³¨å†Œå–æ¶ˆä»¤ç‰Œ
    let cancellation_token = cancellation_manager::register_cancellation_token(execution_id.clone()).await;

    // ä» options ä¸­æå–é…ç½®
    let options = request.options.unwrap_or_default();
    let mut config = ReactConfig::default();
    let max_iterations = config.max_iterations; // ä¿å­˜ç”¨äºè¶…æ—¶è®¡ç®—

    if let Some(max_iter) = options.get("max_iterations").and_then(|v| v.as_u64()) {
        config.max_iterations = max_iter as u32;
    }
    if let Some(temp) = options.get("temperature").and_then(|v| v.as_f64()) {
        config.temperature = Some(temp as f32);
    }
    if let Some(max_tok) = options.get("max_tokens").and_then(|v| v.as_u64()) {
        config.max_tokens = Some(max_tok as u32);
    }
    if let Some(rag) = options.get("enable_rag").and_then(|v| v.as_bool()) {
        config.enable_rag = rag;
    }
    if let Some(verbose) = options.get("verbose").and_then(|v| v.as_bool()) {
        config.verbose = verbose;
    }

    // **å…³é”®ä¿®å¤**: ä» options ä¸­è¯»å– tools_allow å¹¶è®¾ç½®åˆ° ReactConfig.allowed_tools
    // è¿™æ · ReAct executor çš„ build_tools_information æ‰èƒ½è¯»å–åˆ°æ­£ç¡®çš„å·¥å…·ç™½åå•
    if let Some(tools_allow) = options.get("tools_allow") {
        if let Some(arr) = tools_allow.as_array() {
            let tool_names: Vec<String> = arr.iter()
                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                .collect();
            log::info!("ReAct dispatch: è®¾ç½® allowed_tools = {:?}", tool_names);
            config.allowed_tools = Some(tool_names);
        }
    }

    // è·å–é»˜è®¤ AI æœåŠ¡
    let ai_service = match ai_service_manager.get_default_chat_model().await {
        Ok(Some((provider, model))) => {
            match ai_service_manager.get_provider_config(&provider).await {
                Ok(Some(mut provider_config)) => {
                    provider_config.model = model;
                    let mcp_service = ai_service_manager.get_mcp_service();
                    let ai_svc = crate::services::ai::AiService::new(
                        provider_config,
                        db_service.clone(),
                        Some(app.clone()),
                        mcp_service.clone(),
                    );
                    Arc::new(ai_svc)
                }
                _ => {
                    // Provideré…ç½®è·å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ "default" æœåŠ¡
                    log::warn!("Failed to get provider config for '{}', falling back to 'default' service", provider);
                    match ai_service_manager.get_service("default") {
                        Some(service) => Arc::new(service),
                        None => {
                            return Err(format!("Failed to get AI provider config for '{}' and no default service available", provider));
                        }
                    }
                }
            }
        }
        _ => {
            // æ²¡æœ‰é…ç½®é»˜è®¤æ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨ "default" æœåŠ¡
            log::warn!("No default chat model configured, trying to use 'default' service");
            match ai_service_manager.get_service("default") {
                Some(service) => Arc::new(service),
                None => {
                    return Err("No default AI model configured and no default service available".to_string());
                }
            }
        }
    };

    // åºåˆ—åŒ– config
    let config_json = serde_json::to_value(&config).map_err(|e| e.to_string())?;

    // åˆ›å»º ReactEngine
    let engine = ReactEngine::new(config).with_services(
        ai_service,
        ai_service_manager.get_mcp_service(),
        Some(db_service.clone()),
        Some(app.clone()),
    );

    // åˆ›å»º AgentTask
    let task = AgentTask {
        id: execution_id.clone(),
        description: request.query.clone(),
        target: None,
        parameters: {
            let mut map = HashMap::new();
            map.insert("query".to_string(), serde_json::json!(request.query));
            map.insert("config".to_string(), config_json);

            // **å…³é”®ä¿®å¤**: å°† tools_allow å’Œ tools_deny ä» options é€ä¼ åˆ° task.parameters é¡¶å±‚
            // ReAct executor çš„ build_tools_information ä¼šä»è¿™é‡Œè¯»å–
            if let Some(tools_allow) = options.get("tools_allow") {
                log::info!("ReAct dispatch: é€ä¼  tools_allow åˆ° task.parameters");
                map.insert("tools_allow".to_string(), tools_allow.clone());
            }
            if let Some(tools_deny) = options.get("tools_deny") {
                log::info!("ReAct dispatch: é€ä¼  tools_deny åˆ° task.parameters");
                map.insert("tools_deny".to_string(), tools_deny.clone());
            }

            // æ·»åŠ  conversation_id å’Œ message_id åˆ° parametersï¼Œè®© ReAct å¼•æ“èƒ½å¤Ÿæå–
            if let Some(conv_id) = &request.conversation_id {
                map.insert("conversation_id".to_string(), serde_json::json!(conv_id));
            }
            if let Some(msg_id) = &request.message_id {
                map.insert("message_id".to_string(), serde_json::json!(msg_id));
            }
            map
        },
        user_id: "default".to_string(),
        priority: TaskPriority::Normal,
        timeout: Some(max_iterations as u64 * 30000), // 30s per iteration
    };

    // åˆ›å»º dummy session ç”¨äºæ‰§è¡Œ
    use crate::agents::traits::{AgentSession, AgentSessionStatus, LogLevel, AgentExecutionResult, SessionLog};
    struct DummySession {
        task: AgentTask,
        status: AgentSessionStatus,
        logs: Vec<SessionLog>,
        result: Option<AgentExecutionResult>,
    }

    #[async_trait::async_trait]
    impl AgentSession for DummySession {
        fn get_session_id(&self) -> &str { "dummy" }
        fn get_task(&self) -> &AgentTask { &self.task }
        fn get_status(&self) -> AgentSessionStatus { self.status.clone() }
        async fn update_status(&mut self, status: AgentSessionStatus) -> anyhow::Result<()> {
            self.status = status;
            Ok(())
        }
        async fn add_log(&mut self, level: LogLevel, message: String) -> anyhow::Result<()> {
            self.logs.push(SessionLog {
                level,
                message,
                timestamp: chrono::Utc::now(),
                source: "react".to_string(),
            });
            Ok(())
        }
        fn get_logs(&self) -> &[SessionLog] { &self.logs }
        async fn set_result(&mut self, result: AgentExecutionResult) -> anyhow::Result<()> {
            self.result = Some(result);
            Ok(())
        }
        fn get_result(&self) -> Option<&AgentExecutionResult> { self.result.as_ref() }
    }

    let mut session = DummySession {
        task,
        status: AgentSessionStatus::Executing,
        logs: Vec::new(),
        result: None,
    };

    // æ‰§è¡Œä»»åŠ¡ - å…ˆå…‹éš† task é¿å…å€Ÿç”¨å†²çª
    let task_clone = session.task.clone();
    let start_time = std::time::Instant::now();
    match engine.execute(&task_clone, &mut session).await {
        Ok(result) => {
            let duration_ms = start_time.elapsed().as_millis() as u64;
            // ä» result.data ä¸­æå–å“åº”æ–‡æœ¬
            let response = if let Some(data) = &result.data {
                data.as_str().unwrap_or("").to_string()
            } else {
                "ReAct execution completed".to_string()
            };

            Ok(DispatchResult {
                execution_id,
                initial_response: response,
                execution_plan: None,
                estimated_duration: duration_ms,
                selected_architecture: "ReAct".to_string(),
            })
        }
        Err(e) => Err(format!("ReAct execution failed: {}", e))
    }
}

// dispatch_with_rewoo, dispatch_with_llm_compiler, dispatch_with_travel å·²åˆ é™¤
// è¿™äº›èƒ½åŠ›ç°åœ¨å†…åµŒåœ¨æ³›åŒ–çš„ ReAct å¼•æ“ä¸­

/// ä½¿ç”¨ LLM ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–ç›®æ ‡ä¿¡æ¯å’Œä»»åŠ¡ç±»å‹
async fn extract_target_with_llm(
    query: &str,
    ai_service: &Arc<crate::services::ai::AiService>,
) -> (Option<String>, String, String) {
    let system_prompt = r#"ä½ æ˜¯ä¸€ä¸ªå®‰å…¨æµ‹è¯•ä»»åŠ¡åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·è¾“å…¥ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼ˆåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š
{
  "task_type": "ä»»åŠ¡ç±»å‹",
  "target": "ç›®æ ‡å¯¹è±¡",
  "target_type": "ç›®æ ‡ç±»å‹"
}

**ä»»åŠ¡ç±»å‹(task_type)é€‰é¡¹ï¼š**
- web_pentest: Webæ¸—é€æµ‹è¯•ï¼ˆç½‘ç«™å®‰å…¨æµ‹è¯•ã€æ¼æ´æ‰«æï¼‰
- api_pentest: APIå®‰å…¨æµ‹è¯•ï¼ˆREST APIã€GraphQLæµ‹è¯•ï¼‰
- code_audit: ä»£ç å®¡è®¡ï¼ˆæºç å®¡è®¡ã€SASTã€ä»£ç æ‰«æï¼‰
- ctf: CTFå¤ºæ——èµ›ï¼ˆè§£é¢˜ã€challengeï¼‰
- reverse_engineering: é€†å‘å·¥ç¨‹ï¼ˆäºŒè¿›åˆ¶åˆ†æã€åç¼–è¯‘ï¼‰
- forensics: æ•°å­—å–è¯ï¼ˆæ—¥å¿—åˆ†æã€äº‹ä»¶è°ƒæŸ¥ï¼‰
- mobile_security: ç§»åŠ¨åº”ç”¨å®‰å…¨ï¼ˆAndroid/iOSæµ‹è¯•ï¼‰
- cloud_security: äº‘å®‰å…¨è¯„ä¼°ï¼ˆAWS/Azure/GCPé…ç½®å®¡è®¡ï¼‰
- iot_security: ç‰©è”ç½‘/å·¥æ§å®‰å…¨ï¼ˆæ™ºèƒ½è®¾å¤‡ã€SCADAï¼‰
- network_pentest: ç½‘ç»œæ¸—é€ï¼ˆå†…ç½‘æ¸—é€ã€ç«¯å£æ‰«æï¼‰
- social_engineering: ç¤¾ä¼šå·¥ç¨‹å­¦ï¼ˆé’“é±¼æµ‹è¯•ï¼‰
- other: å…¶ä»–å®‰å…¨æµ‹è¯•

**ç›®æ ‡ç±»å‹(target_type)é€‰é¡¹ï¼š**
- url: HTTP/HTTPSç½‘å€
- file_path: æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
- github_repo: GitHubä»“åº“ï¼ˆowner/repoæ ¼å¼ï¼‰
- ip_address: IPåœ°å€æˆ–IPæ®µï¼ˆCIDRï¼‰
- domain: åŸŸå
- binary_file: äºŒè¿›åˆ¶æ–‡ä»¶
- mobile_app: ç§»åŠ¨åº”ç”¨ï¼ˆåŒ…åæˆ–APK/IPAï¼‰
- cloud_resource: äº‘èµ„æºæ ‡è¯†
- none: æ— æ˜ç¡®ç›®æ ‡

**æå–è§„åˆ™ï¼š**
1. è¯†åˆ«æŸ¥è¯¢ä¸­çš„å…³é”®è¯ï¼Œåˆ¤æ–­ä»»åŠ¡ç±»å‹
2. æå–å…·ä½“çš„ç›®æ ‡å¯¹è±¡ï¼ˆURLã€è·¯å¾„ã€ä»“åº“ç­‰ï¼‰
3. å¦‚æœæ²¡æœ‰æ˜ç¡®ç›®æ ‡ï¼Œtargetè®¾ä¸ºnull
4. ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼Œç¡®ä¿å¯è§£æ

ç¤ºä¾‹ï¼š
- "å¯¹ http://example.com è¿›è¡Œæ¸—é€æµ‹è¯•" â†’ {"task_type":"web_pentest","target":"http://example.com","target_type":"url"}
- "å®¡è®¡ /path/to/code çš„ä»£ç " â†’ {"task_type":"code_audit","target":"/path/to/code","target_type":"file_path"}
- "è§£è¿™é“CTFé¢˜" â†’ {"task_type":"ctf","target":null,"target_type":"none"}"#;

    let user_prompt = format!(r#"ç”¨æˆ·è¾“å…¥ï¼š"{}"

è¯·æå–ä»»åŠ¡ç±»å‹ã€ç›®æ ‡å’Œç›®æ ‡ç±»å‹ï¼Œè¿”å›JSONæ ¼å¼ã€‚"#, query);

    // ä½¿ç”¨ç»Ÿä¸€çš„ LlmClient
    let llm_client = crate::engines::create_client(&ai_service);
    
    match llm_client.completion(Some(system_prompt), &user_prompt).await {
        Ok(response) => {
            log::debug!("LLM extraction response: {}", response);
            
            // å°è¯•ä»å“åº”ä¸­æå–JSONï¼ˆå¯èƒ½åŒ…å«markdownä»£ç å—ï¼‰
            let json_str: String = if response.contains("```json") {
                // æå– ```json ... ``` ä¸­çš„å†…å®¹
                if let Some(start) = response.find("```json") {
                    let json_start = start + 7; // "```json".len()
                    if let Some(end_pos) = response[json_start..].find("```") {
                        response[json_start..json_start + end_pos].trim().to_string()
                    } else {
                        response.trim().to_string()
                    }
                } else {
                    response.trim().to_string()
                }
            } else if response.contains("```") {
                // æå– ``` ... ``` ä¸­çš„å†…å®¹
                if let Some(start) = response.find("```") {
                    let content_start = start + 3;
                    if let Some(end_pos) = response[content_start..].find("```") {
                        response[content_start..content_start + end_pos].trim().to_string()
                    } else {
                        response.trim().to_string()
                    }
                } else {
                    response.trim().to_string()
                }
            } else {
                response.trim().to_string()
            };
            
            // è§£æ JSON
            if let Ok(json) = serde_json::from_str::<serde_json::Value>(&json_str) {
                let task_type = json.get("task_type")
                    .and_then(|v| v.as_str())
                    .unwrap_or("other")
                    .to_string();
                
                let target = json.get("target")
                    .and_then(|v| {
                        if v.is_null() {
                            None
                        } else {
                            v.as_str().map(|s| s.to_string())
                        }
                    });
                
                let target_type = json.get("target_type")
                    .and_then(|v| v.as_str())
                    .unwrap_or("none")
                    .to_string();
                
                log::info!("âœ… LLM extraction - task_type: {}, target: {:?}, target_type: {}", 
                    task_type, target, target_type);
                
                return (target, task_type, target_type);
            } else {
                log::warn!("Failed to parse LLM response as JSON: {}", json_str);
            }
        }
        Err(e) => {
            log::error!("Failed to call LLM for target extraction: {}", e);
        }
    }
    
    // é™çº§ï¼šä½¿ç”¨ç®€å•çš„æ­£åˆ™æå–
    log::info!("âš ï¸ Falling back to regex-based extraction");
    fallback_extract_target(query)
}

/// é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç›®æ ‡
fn fallback_extract_target(query: &str) -> (Option<String>, String, String) {
    let query_lower = query.to_lowercase();
    
    // 1. å°è¯•æå– URL
    if let Ok(url_regex) = regex::Regex::new(r"https?://[^\s]+") {
        if let Some(m) = url_regex.find(query) {
            let url = m.as_str().to_string();
            let task_type = if query_lower.contains("api") {
                "api_pentest"
            } else {
                "web_pentest"
            };
            return (Some(url), task_type.to_string(), "url".to_string());
        }
    }
    
    // 2. å°è¯•æå–æ–‡ä»¶è·¯å¾„
    let path_patterns = vec![
        r"/[^\s]+",                 // Unix è·¯å¾„
        r"[A-Z]:\\[^\s]+",          // Windows è·¯å¾„
        r"\./[^\s]+",               // ç›¸å¯¹è·¯å¾„
        r"~/[^\s]+",                // Home è·¯å¾„
    ];
    
    for pattern in path_patterns {
        if let Ok(regex) = regex::Regex::new(pattern) {
            if let Some(m) = regex.find(query) {
                let path = m.as_str().to_string();
                let task_type = if query_lower.contains("ä»£ç ") || query_lower.contains("code") || query_lower.contains("å®¡è®¡") {
                    "code_audit"
                } else if query_lower.contains("ctf") || query_lower.contains("é¢˜") {
                    "ctf"
                } else if query_lower.contains("é€†å‘") || query_lower.contains("reverse") {
                    "reverse_engineering"
                } else if query_lower.contains("å–è¯") || query_lower.contains("forensics") || query_lower.contains("æ—¥å¿—") {
                    "forensics"
                } else {
                    "other"
                };
                return (Some(path), task_type.to_string(), "file_path".to_string());
            }
        }
    }
    
    // 3. å°è¯•æå– GitHub ä»“åº“
    if let Ok(regex) = regex::Regex::new(r"github\.com/([a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+)") {
        if let Some(captures) = regex.captures(query) {
            if let Some(repo) = captures.get(1) {
                return (
                    Some(repo.as_str().to_string()),
                    "code_audit".to_string(),
                    "github_repo".to_string()
                );
            }
        }
    }
    
    // 4. å°è¯•æå– IP åœ°å€
    if let Ok(regex) = regex::Regex::new(r"\b(?:\d{1,3}\.){3}\d{1,3}(?:/\d{1,2})?\b") {
        if let Some(m) = regex.find(query) {
            return (
                Some(m.as_str().to_string()),
                "network_pentest".to_string(),
                "ip_address".to_string()
            );
        }
    }
    
    // 5. æ ¹æ®å…³é”®è¯æ¨æ–­ä»»åŠ¡ç±»å‹
    let task_type = if query_lower.contains("ä»£ç ") || query_lower.contains("code") || query_lower.contains("å®¡è®¡") {
        "code_audit"
    } else if query_lower.contains("ctf") || query_lower.contains("å¤ºæ——") {
        "ctf"
    } else if query_lower.contains("é€†å‘") || query_lower.contains("reverse") {
        "reverse_engineering"
    } else if query_lower.contains("å–è¯") || query_lower.contains("forensics") {
        "forensics"
    } else if query_lower.contains("api") {
        "api_pentest"
    } else if query_lower.contains("ç§»åŠ¨") || query_lower.contains("mobile") || query_lower.contains("android") || query_lower.contains("ios") {
        "mobile_security"
    } else if query_lower.contains("äº‘") || query_lower.contains("cloud") || query_lower.contains("aws") || query_lower.contains("azure") {
        "cloud_security"
    } else if query_lower.contains("ç½‘ç»œ") || query_lower.contains("network") || query_lower.contains("å†…ç½‘") {
        "network_pentest"
    } else {
        "other"
    };
    
    // æ²¡æœ‰æ‰¾åˆ°æ˜ç¡®ç›®æ ‡
    (None, task_type.to_string(), "none".to_string())
}

// ============================================================================
// è‡ªå®šä¹‰ AI æä¾›å•†ç›¸å…³å‘½ä»¤
// ============================================================================

/// æµ‹è¯•è‡ªå®šä¹‰æä¾›å•†è¯·æ±‚å‚æ•°
#[derive(Debug, Serialize, Deserialize)]
pub struct TestCustomProviderRequest {
    pub name: String,
    pub api_key: Option<String>,
    pub api_base: String,
    pub model_id: String,
    pub compat_mode: String, // openai, anthropic, rig_openai, rig_anthropic
    pub extra_headers: Option<std::collections::HashMap<String, String>>,
    pub timeout: Option<u64>,
}

/// æµ‹è¯•è‡ªå®šä¹‰æä¾›å•†å“åº”
#[derive(Debug, Serialize, Deserialize)]
pub struct TestCustomProviderResponse {
    pub success: bool,
    pub message: String,
}

/// æ·»åŠ è‡ªå®šä¹‰æä¾›å•†è¯·æ±‚å‚æ•°
#[derive(Debug, Serialize, Deserialize)]
pub struct AddCustomProviderRequest {
    pub name: String,
    pub display_name: String,
    pub api_key: Option<String>,
    pub api_base: String,
    pub model_id: String,
    pub compat_mode: String,
    pub extra_headers: Option<std::collections::HashMap<String, String>>,
    pub timeout: Option<u64>,
    pub max_retries: Option<u32>,
}

/// æµ‹è¯•è‡ªå®šä¹‰ AI æä¾›å•†è¿æ¥
#[tauri::command]
pub async fn test_custom_provider(
    request: TestCustomProviderRequest,
) -> Result<TestCustomProviderResponse, String> {
    info!("Testing custom provider: {} (mode: {})", request.name, request.compat_mode);
    
    // ä½¿ç”¨ rig-core æµ‹è¯•æ‰€æœ‰æä¾›å•†
    let result = test_with_rig(&request).await;
    
    match result {
        Ok(msg) => Ok(TestCustomProviderResponse {
            success: true,
            message: msg,
        }),
        Err(e) => Ok(TestCustomProviderResponse {
            success: false,
            message: format!("Connection test failed: {}", e),
        }),
    }
}

/// ä½¿ç”¨ rig-core æµ‹è¯•è¿æ¥
async fn test_with_rig(request: &TestCustomProviderRequest) -> Result<String, String> {
    use rig::client::{CompletionClient, ProviderClient};
    use rig::completion::Prompt;
    
    let provider = if request.compat_mode == "rig_anthropic" {
        "anthropic"
    } else {
        "openai"
    };
    
    // è®¾ç½®ç¯å¢ƒå˜é‡
    if let Some(api_key) = &request.api_key {
        match provider {
            "anthropic" => {
                std::env::set_var("ANTHROPIC_API_KEY", api_key);
                std::env::set_var("ANTHROPIC_API_BASE", &request.api_base);
            }
            _ => {
                std::env::set_var("OPENAI_API_KEY", api_key);
                std::env::set_var("OPENAI_API_BASE", &request.api_base);
                std::env::set_var("OPENAI_BASE_URL", &request.api_base);
            }
        }
    }
    
    let timeout = std::time::Duration::from_secs(request.timeout.unwrap_or(30));
    
    // æ ¹æ® provider åˆ›å»º agent
    let response = if provider == "anthropic" {
        use rig::providers::anthropic;
        let client = anthropic::Client::from_env();
        let agent = client.agent(&request.model_id).max_tokens(1024).build();
        tokio::time::timeout(timeout, agent.prompt("Hello, respond with 'OK' if you receive this."))
            .await
            .map_err(|_| "Request timeout".to_string())?
            .map_err(|e| format!("Request failed: {}", e))?
    } else {
        use rig::providers::openai;
        let client = openai::Client::from_env();
        let agent = client.agent(&request.model_id).build();
        tokio::time::timeout(timeout, agent.prompt("Hello, respond with 'OK' if you receive this."))
            .await
            .map_err(|_| "Request timeout".to_string())?
            .map_err(|e| format!("Request failed: {}", e))?
    };
    
    Ok(format!("Connection successful! Response: {}", response.chars().take(100).collect::<String>()))
}

/// æ·»åŠ è‡ªå®šä¹‰ AI æä¾›å•†
#[tauri::command]
pub async fn add_custom_provider(
    request: AddCustomProviderRequest,
    db_service: State<'_, Arc<DatabaseService>>,
    ai_manager: State<'_, Arc<AiServiceManager>>,
) -> Result<(), String> {
    info!("Adding custom provider: {} ({})", request.name, request.display_name);
    
    // è·å–ç°æœ‰çš„ providers_config
    let mut providers: serde_json::Map<String, serde_json::Value> = 
        match db_service.get_config("ai", "providers_config").await {
            Ok(Some(json_str)) => {
                serde_json::from_str(&json_str).unwrap_or_default()
            }
            _ => serde_json::Map::new(),
        };
    
    // æ„å»ºæ–°æä¾›å•†é…ç½®
    let provider_id = request.name.to_lowercase().replace(" ", "_");
    let new_provider = serde_json::json!({
        "id": provider_id,
        "provider": provider_id,
        "name": request.display_name,
        "enabled": true,
        "api_key": request.api_key,
        "api_base": request.api_base,
        "organization": null,
        "default_model": request.model_id,
        "compat_mode": request.compat_mode,
        "extra_headers": request.extra_headers,
        "timeout": request.timeout.unwrap_or(120),
        "max_retries": request.max_retries.unwrap_or(3),
        "is_custom": true,
        "models": [{
            "id": request.model_id,
            "name": request.model_id,
            "description": format!("Custom model from {}", request.display_name),
            "context_length": 4096,
            "supports_streaming": true,
            "supports_tools": false,
            "supports_vision": false,
            "is_available": true
        }]
    });
    
    // æ·»åŠ åˆ°é…ç½®
    providers.insert(request.name.clone(), new_provider);
    
    // ä¿å­˜åˆ°æ•°æ®åº“
    let config_str = serde_json::to_string(&providers)
        .map_err(|e| format!("Failed to serialize providers config: {}", e))?;
    
    db_service
        .set_config(
            "ai",
            "providers_config",
            &config_str,
            Some("AI providers configuration"),
        )
        .await
        .map_err(|e| format!("Failed to save providers config: {}", e))?;
    
    // å¦‚æœæœ‰ API Keyï¼Œå•ç‹¬ä¿å­˜ï¼ˆåŠ å¯†å­˜å‚¨ï¼‰
    if let Some(api_key) = &request.api_key {
        if !api_key.is_empty() {
            let key_name = format!("api_key_{}", provider_id);
            db_service
                .set_config("ai", &key_name, api_key, Some(&format!("{} API key", request.display_name)))
                .await
                .map_err(|e| format!("Failed to save API key: {}", e))?;
        }
    }
    
    // é‡æ–°åŠ è½½ AI æœåŠ¡
    if let Err(e) = ai_manager.reload_services().await {
        warn!("Failed to reload AI services after adding custom provider: {}", e);
    }
    
    info!("Custom provider '{}' added successfully", request.name);
    Ok(())
}

// ============================================================================
// Aliyun DashScope Commands
// ============================================================================

/// æµ‹è¯•é˜¿é‡Œäº‘ DashScope è¿æ¥
#[tauri::command]
pub async fn test_aliyun_dashscope_connection(
    api_key: String,
    model: String,
) -> Result<bool, String> {
    use crate::utils::aliyun_oss::test_dashscope_connection;
    
    info!("Testing Aliyun DashScope connection with model: {}", model);
    
    test_dashscope_connection(&api_key, &model)
        .await
        .map_err(|e| format!("Connection test failed: {}", e))
}

/// ä¸Šä¼ æ–‡ä»¶åˆ°é˜¿é‡Œäº‘ OSS
#[tauri::command]
pub async fn upload_file_to_aliyun(
    api_key: String,
    model: String,
    file_path: String,
) -> Result<crate::utils::aliyun_oss::UploadResult, String> {
    use crate::utils::aliyun_oss::upload_file_and_get_url;
    use std::path::Path;
    
    info!("Uploading file to Aliyun OSS: {}", file_path);
    
    let path = Path::new(&file_path);
    if !path.exists() {
        return Err(format!("File not found: {}", file_path));
    }
    
    upload_file_and_get_url(&api_key, &model, path)
        .await
        .map_err(|e| format!("Upload failed: {}", e))
}

/// ä½¿ç”¨æ•°æ®åº“é…ç½®ä¸Šä¼ æ–‡ä»¶åˆ°é˜¿é‡Œäº‘ OSS
#[tauri::command]
pub async fn upload_file_to_aliyun_with_config(
    db: tauri::State<'_, Arc<DatabaseService>>,
    file_path: String,
) -> Result<crate::utils::aliyun_oss::UploadResult, String> {
    use crate::utils::aliyun_oss::upload_file_and_get_url;
    use crate::services::database::Database;
    use std::path::Path;
    
    // ä»æ•°æ®åº“è¯»å–é…ç½®
    let api_key = db.get_config("ai", "aliyun_dashscope_api_key")
        .await
        .map_err(|e| format!("Failed to get API key: {}", e))?
        .ok_or("Aliyun DashScope API key not configured")?;
    
    let model = db.get_config("ai", "aliyun_dashscope_model")
        .await
        .map_err(|e| format!("Failed to get model: {}", e))?
        .unwrap_or_else(|| "qwen-vl-plus".to_string());
    
    info!("Uploading file to Aliyun OSS with saved config: {}", file_path);
    
    let path = Path::new(&file_path);
    if !path.exists() {
        return Err(format!("File not found: {}", file_path));
    }
    
    upload_file_and_get_url(&api_key, &model, path)
        .await
        .map_err(|e| format!("Upload failed: {}", e))
}
