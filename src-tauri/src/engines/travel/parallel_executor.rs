//! å¹¶è¡Œæ‰§è¡Œå™¨ - æ”¯æŒ DAG ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ
//!
//! å€Ÿé‰´ LLMCompiler çš„å¹¶è¡Œæ‰§è¡Œèƒ½åŠ›

use super::dag_planner::DagPlanner;
use super::types::*;
use crate::tools::{FrameworkToolAdapter, UnifiedToolCall};
use crate::utils::ordered_message::{emit_message_chunk_arc, ArchitectureType, ChunkType};
use anyhow::{anyhow, Result};
use futures::future::join_all;
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime};
use tokio::sync::{Mutex, RwLock, Semaphore};
use tokio::time::timeout;
use tokio_util::sync::CancellationToken;

/// å¹¶è¡Œæ‰§è¡Œå™¨
pub struct ParallelExecutor {
    /// é…ç½®
    config: ParallelExecutionConfig,
    /// å·¥å…·é€‚é…å™¨
    tool_adapter: Option<Arc<dyn FrameworkToolAdapter>>,
    /// å¹¶å‘æ§åˆ¶ä¿¡å·é‡
    semaphore: Arc<Semaphore>,
    /// ä»»åŠ¡ç»“æœå­˜å‚¨
    task_results: Arc<RwLock<HashMap<String, serde_json::Value>>>,
    /// å–æ¶ˆä»¤ç‰Œ
    cancellation_token: Option<CancellationToken>,
    /// æ¶ˆæ¯å‘é€ç›¸å…³
    app_handle: Option<Arc<tauri::AppHandle>>,
    execution_id: Option<String>,
    message_id: Option<String>,
    conversation_id: Option<String>,
}

impl ParallelExecutor {
    pub fn new(config: ParallelExecutionConfig) -> Self {
        let semaphore = Arc::new(Semaphore::new(config.max_concurrency));
        Self {
            config,
            tool_adapter: None,
            semaphore,
            task_results: Arc::new(RwLock::new(HashMap::new())),
            cancellation_token: None,
            app_handle: None,
            execution_id: None,
            message_id: None,
            conversation_id: None,
        }
    }

    pub fn with_tool_adapter(mut self, adapter: Arc<dyn FrameworkToolAdapter>) -> Self {
        self.tool_adapter = Some(adapter);
        self
    }

    pub fn with_cancellation_token(mut self, token: CancellationToken) -> Self {
        self.cancellation_token = Some(token);
        self
    }

    pub fn with_message_context(
        mut self,
        app_handle: Arc<tauri::AppHandle>,
        execution_id: String,
        message_id: String,
        conversation_id: Option<String>,
    ) -> Self {
        self.app_handle = Some(app_handle);
        self.execution_id = Some(execution_id);
        self.message_id = Some(message_id);
        self.conversation_id = conversation_id;
        self
    }

    /// å‘é€æ¶ˆæ¯åˆ°å‰ç«¯
    fn emit_message(&self, chunk_type: ChunkType, content: &str, structured_data: Option<serde_json::Value>) {
        if let (Some(app_handle), Some(execution_id), Some(message_id)) =
            (&self.app_handle, &self.execution_id, &self.message_id)
        {
            emit_message_chunk_arc(
                app_handle,
                execution_id,
                message_id,
                self.conversation_id.as_deref(),
                chunk_type,
                content,
                false,
                Some("DagExecutor"),
                None,
                Some(ArchitectureType::Travel),
                structured_data,
            );
        }
    }

    /// æ‰§è¡Œ DAG è®¡åˆ’
    pub async fn execute_dag(&self, plan: &mut DagPlan) -> Result<DagExecutionResult> {
        let start_time = Instant::now();
        let mut metrics = DagExecutionMetrics::default();
        metrics.total_tasks = plan.tasks.len() as u32;

        self.emit_message(
            ChunkType::Thinking,
            &format!("[START] Starting DAG execution with {} tasks", plan.tasks.len()),
            Some(serde_json::json!({
                "total_tasks": plan.tasks.len(),
                "max_concurrency": self.config.max_concurrency
            })),
        );

        // æ¸…ç©ºä¹‹å‰çš„ç»“æœ
        {
            let mut results = self.task_results.write().await;
            results.clear();
        }

        // æŒ‰å±‚æ‰§è¡Œ (æ‹“æ‰‘æ’åº)
        let mut completed: Vec<String> = Vec::new();
        let mut failed: Vec<String> = Vec::new();
        let mut current_parallel = 0u32;

        loop {
            // æ£€æŸ¥å–æ¶ˆ
            if let Some(token) = &self.cancellation_token {
                if token.is_cancelled() {
                    log::info!("ParallelExecutor: Execution cancelled");
                    self.emit_message(ChunkType::Error, "[CANCELLED] Execution cancelled", None);
                    break;
                }
            }

            // è·å–å¯æ‰§è¡Œçš„ä»»åŠ¡
            let ready_tasks = self.get_ready_tasks(plan, &completed, &failed);

            if ready_tasks.is_empty() {
                // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å·²å¤„ç†
                let total_processed = completed.len() + failed.len();
                if total_processed >= plan.tasks.len() {
                    break;
                }
                // å¯èƒ½æœ‰å¾ªç¯ä¾èµ–æˆ–æ‰€æœ‰å‰©ä½™ä»»åŠ¡éƒ½ä¾èµ–å¤±è´¥çš„ä»»åŠ¡
                log::warn!("ParallelExecutor: No ready tasks but {} tasks remaining", 
                    plan.tasks.len() - total_processed);
                break;
            }

            // æ›´æ–°æœ€å¤§å¹¶è¡Œæ•°
            current_parallel = ready_tasks.len() as u32;
            if current_parallel > metrics.max_parallel {
                metrics.max_parallel = current_parallel;
            }

            self.emit_message(
                ChunkType::Content,
                &format!("âš¡ Executing {} tasks in parallel", ready_tasks.len()),
                Some(serde_json::json!({
                    "parallel_count": ready_tasks.len(),
                    "completed": completed.len(),
                    "failed": failed.len()
                })),
            );

            // æå–ä»»åŠ¡æ•°æ®ç”¨äºå¹¶è¡Œæ‰§è¡Œ
            let task_data: Vec<_> = ready_tasks
                .iter()
                .filter_map(|task_id| {
                    plan.tasks.iter().find(|t| t.id == *task_id).map(|t| {
                        (t.id.clone(), t.tool_name.clone(), t.arguments.clone())
                    })
                })
                .collect();

            // æ ‡è®°ä»»åŠ¡ä¸ºè¿è¡Œä¸­
            for task_id in &ready_tasks {
                if let Some(task) = plan.tasks.iter_mut().find(|t| t.id == *task_id) {
                    task.status = DagTaskStatus::Running;
                    task.started_at = Some(SystemTime::now());
                }
            }

            // å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
            let task_futures: Vec<_> = task_data
                .into_iter()
                .map(|(task_id, tool_name, arguments)| {
                    self.execute_task_by_data(task_id, tool_name, arguments)
                })
                .collect();

            let results = join_all(task_futures).await;

            // å¤„ç†ç»“æœ
            for (task_id, result) in results {
                match result {
                    Ok(output) => {
                        completed.push(task_id.clone());
                        metrics.completed_tasks += 1;

                        // å­˜å‚¨ç»“æœä¾›åç»­ä»»åŠ¡å¼•ç”¨
                        {
                            let mut stored = self.task_results.write().await;
                            stored.insert(task_id.clone(), output.clone());
                        }

                        // æ›´æ–°ä»»åŠ¡çŠ¶æ€
                        if let Some(task) = plan.tasks.iter_mut().find(|t| t.id == task_id) {
                            task.status = DagTaskStatus::Completed;
                            task.result = Some(output);
                            task.completed_at = Some(SystemTime::now());
                        }
                    }
                    Err(e) => {
                        failed.push(task_id.clone());
                        metrics.failed_tasks += 1;

                        self.emit_message(
                            ChunkType::Error,
                            &format!("[FAILED] Task {} failed: {}", task_id, e),
                            None,
                        );

                        // æ›´æ–°ä»»åŠ¡çŠ¶æ€
                        if let Some(task) = plan.tasks.iter_mut().find(|t| t.id == task_id) {
                            task.status = DagTaskStatus::Failed;
                            task.error = Some(e.to_string());
                            task.completed_at = Some(SystemTime::now());
                        }
                    }
                }
            }
        }

        // æ ‡è®°å› ä¾èµ–å¤±è´¥è€Œè·³è¿‡çš„ä»»åŠ¡
        for task in plan.tasks.iter_mut() {
            if task.status == DagTaskStatus::Pending || task.status == DagTaskStatus::Ready {
                task.status = DagTaskStatus::Skipped;
                metrics.skipped_tasks += 1;
            }
        }

        metrics.total_duration_ms = start_time.elapsed().as_millis() as u64;

        // æ”¶é›†æ‰€æœ‰ä»»åŠ¡ç»“æœ
        let task_results = self.task_results.read().await.clone();

        // è®¡ç®—èŠ‚çœçš„ Token (ä¼°ç®—: æ¯çœç•¥ä¸€æ¬¡ LLM è°ƒç”¨çº¦èŠ‚çœ 500 tokens)
        // ä¼ ç»Ÿ ReAct: æ¯ä¸ªä»»åŠ¡éœ€è¦ Thought+Action+Observation ä¸‰æ¬¡äº¤äº’
        // DAG æ¨¡å¼: åªéœ€è¦ä¸€æ¬¡è§„åˆ’
        metrics.tokens_saved = (metrics.total_tasks.saturating_sub(1)) * 500;
        metrics.llm_calls = 1; // DAG æ¨¡å¼åªéœ€è¦ä¸€æ¬¡ LLM è°ƒç”¨è§„åˆ’

        let success = failed.is_empty();

        self.emit_message(
            ChunkType::Content,
            &format!(
                "ğŸ“Š DAG execution completed: {} succeeded, {} failed, {} skipped",
                metrics.completed_tasks, metrics.failed_tasks, metrics.skipped_tasks
            ),
            Some(serde_json::json!({
                "success": success,
                "metrics": metrics
            })),
        );

        Ok(DagExecutionResult {
            plan_id: plan.id.clone(),
            success,
            task_results,
            failed_tasks: failed,
            metrics,
            final_output: self.build_final_output(plan).await,
            // v3.0 å¢å¼ºå­—æ®µ
            needs_replanning: false,
            replan_reason: None,
            execution_snapshot: None,
        })
    }

    /// è·å–å¯æ‰§è¡Œçš„ä»»åŠ¡ (ä¾èµ–å·²æ»¡è¶³)
    fn get_ready_tasks(&self, plan: &DagPlan, completed: &[String], failed: &[String]) -> Vec<String> {
        plan.tasks
            .iter()
            .filter(|task| {
                // å¿…é¡»æ˜¯ Pending çŠ¶æ€
                if task.status != DagTaskStatus::Pending {
                    return false;
                }
                // ä¾èµ–å¿…é¡»å…¨éƒ¨å®Œæˆä¸”æ²¡æœ‰å¤±è´¥
                task.depends_on.iter().all(|dep| {
                    completed.contains(dep) && !failed.contains(dep)
                })
            })
            .map(|t| t.id.clone())
            .collect()
    }

    /// æ‰§è¡Œå•ä¸ªä»»åŠ¡ (é€šè¿‡æ•°æ®)
    async fn execute_task_by_data(
        &self,
        task_id: String,
        tool_name: String,
        arguments: HashMap<String, serde_json::Value>,
    ) -> (String, Result<serde_json::Value>) {
        log::info!("ParallelExecutor: Executing task {} - {}", task_id, tool_name);

        self.emit_message(
            ChunkType::Content,
            &format!("[TOOL] Executing: {}({})", tool_name, 
                arguments.keys().cloned().collect::<Vec<_>>().join(", ")),
            Some(serde_json::json!({
                "task_id": task_id,
                "tool_name": tool_name,
                "arguments": arguments
            })),
        );

        // è·å–ä¿¡å·é‡è®¸å¯
        let _permit = match self.semaphore.acquire().await {
            Ok(p) => p,
            Err(e) => {
                log::error!("Failed to acquire semaphore: {}", e);
                return (task_id, Err(anyhow!("Semaphore error: {}", e)));
            }
        };

        // è§£æå˜é‡å¼•ç”¨
        let mut resolved_args = arguments.clone();
        {
            let results = self.task_results.read().await;
            DagPlanner::resolve_variable_references(&mut resolved_args, &results);
        }

        // æ‰§è¡Œå·¥å…·
        let result = self.execute_tool(&tool_name, resolved_args).await;

        match &result {
            Ok(_) => {
                self.emit_message(
                    ChunkType::ToolResult,
                    &format!("[SUCCESS] Task {} completed", task_id),
                    Some(serde_json::json!({
                        "task_id": task_id,
                        "tool_name": tool_name,
                        "success": true
                    })),
                );
            }
            Err(e) => {
                log::error!("Task {} failed: {}", task_id, e);
            }
        }

        (task_id, result)
    }

    /// æ‰§è¡Œå·¥å…·è°ƒç”¨
    async fn execute_tool(
        &self,
        tool_name: &str,
        arguments: HashMap<String, serde_json::Value>,
    ) -> Result<serde_json::Value> {
        let unified_call = UnifiedToolCall {
            id: uuid::Uuid::new_v4().to_string(),
            tool_name: tool_name.to_string(),
            parameters: arguments,
            timeout: Some(Duration::from_secs(self.config.task_timeout)),
            context: HashMap::new(),
            retry_count: 0,
        };

        // ä¼˜å…ˆä½¿ç”¨è®¾ç½®çš„ tool_adapter
        if let Some(adapter) = &self.tool_adapter {
            let result = timeout(
                Duration::from_secs(self.config.task_timeout),
                adapter.execute_tool(unified_call),
            )
            .await
            .map_err(|_| anyhow!("Tool execution timeout"))??;

            return Ok(result.output);
        }

        // é™çº§ä½¿ç”¨å…¨å±€ adapter
        let engine_adapter = crate::tools::get_global_engine_adapter()
            .map_err(|e| anyhow!("No tool adapter available: {}", e))?;

        let result = timeout(
            Duration::from_secs(self.config.task_timeout),
            engine_adapter.execute_tool(unified_call),
        )
        .await
        .map_err(|_| anyhow!("Tool execution timeout"))??;

        Ok(result.output)
    }

    /// æ„å»ºæœ€ç»ˆè¾“å‡º
    async fn build_final_output(&self, plan: &DagPlan) -> Option<serde_json::Value> {
        let results = self.task_results.read().await;

        // å¦‚æœåªæœ‰ä¸€ä¸ªä»»åŠ¡ï¼Œç›´æ¥è¿”å›å…¶ç»“æœ
        if plan.tasks.len() == 1 {
            return results.values().next().cloned();
        }

        // åˆå¹¶æ‰€æœ‰ä»»åŠ¡ç»“æœ
        let mut combined = serde_json::Map::new();
        for task in &plan.tasks {
            if let Some(result) = results.get(&task.id) {
                combined.insert(format!("task_{}", task.id), result.clone());
            }
        }

        Some(serde_json::Value::Object(combined))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_get_ready_tasks() {
        let config = ParallelExecutionConfig::default();
        let executor = ParallelExecutor::new(config);

        let mut plan = DagPlan::new("test".to_string());
        plan.add_task(DagTask::new("1".to_string(), "tool1".to_string(), HashMap::new()));
        plan.add_task(
            DagTask::new("2".to_string(), "tool2".to_string(), HashMap::new())
                .with_depends(vec!["1".to_string()]),
        );

        // åˆå§‹çŠ¶æ€: åªæœ‰ä»»åŠ¡1å¯æ‰§è¡Œ
        let ready = executor.get_ready_tasks(&plan, &[], &[]);
        assert_eq!(ready, vec!["1".to_string()]);

        // ä»»åŠ¡1å®Œæˆå: ä»»åŠ¡2å¯æ‰§è¡Œ
        let ready = executor.get_ready_tasks(&plan, &["1".to_string()], &[]);
        assert_eq!(ready, vec!["2".to_string()]);
    }
}

