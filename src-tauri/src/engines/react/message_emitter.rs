//! ReAct æ¶ˆæ¯å‘é€å™¨å’Œä¸“ç”¨ LLM å®¢æˆ·ç«¯
//!
//! ç®€åŒ–ç‰ˆï¼šç›´æ¥å‘é€æµå¼å†…å®¹åˆ°å‰ç«¯ï¼Œå¹¶æ”¶é›†å®Œæ•´å†…å®¹ç”¨äºä¿å­˜

use crate::engines::LlmConfig;
use crate::utils::ordered_message::{emit_message_chunk_with_arch, ArchitectureType, ChunkType};
use anyhow::{anyhow, Result};
use futures::StreamExt;
use rig::agent::MultiTurnStreamItem;
use rig::client::{CompletionClient, ProviderClient};
use rig::completion::Message;
use rig::message::UserContent;
use rig::one_or_many::OneOrMany;
use rig::streaming::{StreamedAssistantContent, StreamingPrompt};
use serde::{Deserialize, Serialize};
use std::fs::OpenOptions;
use std::io::Write;
use std::sync::{Arc, Mutex};
use chrono::Utc;
use tauri::AppHandle;
use tracing::{debug, error, info};

/// ReAct æ¶ˆæ¯å‘é€å™¨
pub struct ReactMessageEmitter {
    app_handle: Arc<AppHandle>,
    execution_id: String,
    message_id: String,
    conversation_id: Option<String>,
    /// æ”¶é›†æ‰€æœ‰å‘é€çš„å†…å®¹ï¼Œç”¨äºä¿å­˜åˆ°æ•°æ®åº“
    content_collector: Arc<Mutex<String>>,
}

/// æ‰§è¡Œç»Ÿè®¡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReactExecutionStats {
    pub total_iterations: u32,
    pub tool_calls_count: u32,
    pub successful_tool_calls: u32,
    pub failed_tool_calls: u32,
    pub total_duration_ms: u64,
    pub status: String,
}

impl ReactMessageEmitter {
    pub fn new(
        app_handle: Arc<AppHandle>,
        execution_id: String,
        message_id: String,
        conversation_id: Option<String>,
    ) -> Self {
        Self {
            app_handle,
            execution_id,
            message_id,
            conversation_id,
            content_collector: Arc::new(Mutex::new(String::new())),
        }
    }

    /// è·å–æ”¶é›†çš„å®Œæ•´å†…å®¹ï¼ˆç”¨äºä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    pub fn get_full_content(&self) -> String {
        self.content_collector.lock().unwrap().clone()
    }

    /// å‘é€æ‰§è¡Œå¼€å§‹ä¿¡å·
    pub fn emit_start(&self, config: Option<serde_json::Value>) {
        self.emit_meta("start", serde_json::json!({
            "type": "start",
            "config": config
        }));
    }

    /// å‘é€æ‰§è¡Œå®Œæˆä¿¡å·
    pub fn emit_complete(&self, stats: ReactExecutionStats) {
        // å‘é€å®Œæˆä¿¡å·ï¼ˆis_final = trueï¼‰
        emit_message_chunk_with_arch(
            &self.app_handle,
            &self.execution_id,
            &self.message_id,
            self.conversation_id.as_deref(),
            ChunkType::Meta,
            "",
            true, // is_final
            Some("complete"),
            None,
            Some(ArchitectureType::ReAct),
            Some(serde_json::json!({
                "type": "complete",
                "statistics": stats
            })),
        );
    }

    /// å‘é€æµå¼å†…å®¹ chunkï¼ˆLLM è¾“å‡ºçš„æ¯ä¸ª tokenï¼‰
    pub fn emit_content(&self, content: &str, is_final: bool) {
        // æ”¶é›†å†…å®¹ç”¨äºä¿å­˜åˆ°æ•°æ®åº“
        if let Ok(mut collector) = self.content_collector.lock() {
            collector.push_str(content);
        }
        
        emit_message_chunk_with_arch(
            &self.app_handle,
            &self.execution_id,
            &self.message_id,
            self.conversation_id.as_deref(),
            ChunkType::Content,
            content,
            is_final,
            None,
            None,
            Some(ArchitectureType::ReAct),
            None,
        );
    }

    /// å‘é€æ€è€ƒå†…å®¹ chunkï¼ˆç”¨äºæ˜¾ç¤º LLM çš„ reasoning è¿‡ç¨‹ï¼‰
    pub fn emit_thinking(&self, content: &str) {
        emit_message_chunk_with_arch(
            &self.app_handle,
            &self.execution_id,
            &self.message_id,
            self.conversation_id.as_deref(),
            ChunkType::Thinking,
            content,
            false,
            None,
            None,
            Some(ArchitectureType::ReAct),
            None,
        );
    }

    /// å‘é€é”™è¯¯æ¶ˆæ¯
    pub fn emit_error(&self, error_message: &str) {
        let content = format!(
            "\n\n---\nâŒ **æ‰§è¡Œé”™è¯¯**\n\n{}\n",
            error_message
        );
        
        // å‘é€å†…å®¹
        emit_message_chunk_with_arch(
            &self.app_handle,
            &self.execution_id,
            &self.message_id,
            self.conversation_id.as_deref(),
            ChunkType::Error,
            &content,
            true, // is_final
            Some("error"),
            None,
            Some(ArchitectureType::ReAct),
            None,
        );
        
        // æ”¶é›†åˆ°å®Œæ•´å†…å®¹
        if let Ok(mut collector) = self.content_collector.lock() {
            collector.push_str(&content);
        }
    }

    /// å‘é€æ­¥éª¤è¿›åº¦æ›´æ–°
    pub fn emit_step_progress(
        &self,
        step_id: &str,
        step_description: &str,
        status: &str,  // "running", "completed", "failed", "skipped"
        completed_count: usize,
        total_count: usize,
    ) {
        let status_icon = match status {
            "completed" => "âœ…",
            "failed" => "âŒ",
            "running" => "ğŸ”„",
            "skipped" => "â­ï¸",
            _ => "â³",
        };
        
        let progress_percent = if total_count > 0 {
            (completed_count * 100) / total_count
        } else {
            0
        };
        
        let content = format!(
            "\nğŸ“Š **è¿›åº¦æ›´æ–°**: [{}] {} {} ({}/{}ï¼Œ{}%)\n",
            step_id, status_icon, step_description, completed_count, total_count, progress_percent
        );
        
        self.emit_content(&content, false);
        
        // å‘é€ç»“æ„åŒ–è¿›åº¦æ•°æ®
        self.emit_step("progress", serde_json::json!({
            "type": "progress",
            "step_id": step_id,
            "step_description": step_description,
            "status": status,
            "completed": completed_count,
            "total": total_count,
            "percent": progress_percent
        }));
    }

    /// å‘é€å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆå†…è” markdown æ ¼å¼ + ç»“æ„åŒ–æ•°æ®ï¼‰
    pub fn emit_tool_call(&self, iteration: u32, tool_name: &str, args: &serde_json::Value) {
        let args_str = serde_json::to_string_pretty(args).unwrap_or_default();
        let content = format!(
            "\n\n---\nğŸ”§ **è°ƒç”¨å·¥å…·: `{}`**\n<details>\n<summary>ğŸ“¥ å‚æ•°</summary>\n\n```json\n{}\n```\n</details>\n",
            tool_name, args_str
        );
        self.emit_content(&content, false);

        // åŒæ—¶å‘é€ç»“æ„åŒ–æ•°æ®ï¼ˆç”¨äºçŠ¶æ€è¿½è¸ªï¼‰
        self.emit_step("action", serde_json::json!({
            "type": "step",
            "step": {
                "index": iteration.saturating_sub(1),
                "action": {
                    "tool": tool_name,
                    "args": args,
                    "status": "running"
                }
            }
        }));
    }

    /// å‘é€å·¥å…·æ‰§è¡Œç»“æœï¼ˆå†…è” markdown æ ¼å¼ + ç»“æ„åŒ–æ•°æ®ï¼‰
    pub fn emit_tool_result(&self, iteration: u32, tool_name: &str, args: &serde_json::Value, result: &serde_json::Value, success: bool, duration_ms: u64) {
        let status_icon = if success { "âœ…" } else { "âŒ" };
        let result_str = serde_json::to_string_pretty(result).unwrap_or_default();
        let content = format!(
            "<details>\n<summary>{} ç»“æœ ({}ms)</summary>\n\n```json\n{}\n```\n</details>\n---\n\n",
            status_icon, duration_ms, result_str
        );
        self.emit_content(&content, false);

        // åŒæ—¶å‘é€ç»“æ„åŒ–æ•°æ®ï¼ˆç”¨äºçŠ¶æ€è¿½è¸ªï¼‰
        let status = if success { "completed" } else { "failed" };
        self.emit_step("observation", serde_json::json!({
            "type": "step",
            "step": {
                "index": iteration.saturating_sub(1),
                "action": {
                    "tool": tool_name,
                    "args": args,
                    "status": status
                },
                "observation": result
            }
        }));
    }

    /// å‘é€æ­¥éª¤æ•°æ®
    fn emit_step(&self, stage: &str, data: serde_json::Value) {
        emit_message_chunk_with_arch(
            &self.app_handle,
            &self.execution_id,
            &self.message_id,
            self.conversation_id.as_deref(),
            ChunkType::Meta,
            "",
            false,
            Some(stage),
            None,
            Some(ArchitectureType::ReAct),
            Some(data),
        );
    }

    // === å†…éƒ¨æ–¹æ³• ===

    fn emit_meta(&self, stage: &str, data: serde_json::Value) {
        emit_message_chunk_with_arch(
            &self.app_handle,
            &self.execution_id,
            &self.message_id,
            self.conversation_id.as_deref(),
            ChunkType::Meta,
            "",
            false,
            Some(stage),
            None,
            Some(ArchitectureType::ReAct),
            Some(data),
        );
    }
}

// ============================================================================
// ReactLlmClient - æµå¼ LLM è°ƒç”¨ï¼ˆæ¯ä¸ª token å‘é€åˆ°å‰ç«¯ï¼‰
// ============================================================================

/// ReAct LLM å®¢æˆ·ç«¯
pub struct ReactLlmClient {
    config: LlmConfig,
    emitter: Arc<ReactMessageEmitter>,
}

impl ReactLlmClient {
    pub fn new(config: LlmConfig, emitter: Arc<ReactMessageEmitter>) -> Self {
        Self { config, emitter }
    }

    /// è®¾ç½® rig åº“æ‰€éœ€çš„ç¯å¢ƒå˜é‡
    fn setup_env_vars(&self) {
        let provider = self.config.provider.to_lowercase();
        
        if let Some(api_key) = &self.config.api_key {
            match provider.as_str() {
                "gemini" | "google" => std::env::set_var("GEMINI_API_KEY", api_key),
                "openai" => std::env::set_var("OPENAI_API_KEY", api_key),
                "anthropic" => std::env::set_var("ANTHROPIC_API_KEY", api_key),
                _ => std::env::set_var("OPENAI_API_KEY", api_key),
            }
        }
        
        if let Some(base_url) = &self.config.base_url {
            match provider.as_str() {
                "gemini" | "google" => std::env::set_var("GEMINI_API_BASE", base_url),
                "anthropic" => std::env::set_var("ANTHROPIC_API_BASE", base_url),
                _ => {
                    std::env::set_var("OPENAI_API_BASE", base_url);
                    std::env::set_var("OPENAI_BASE_URL", base_url);
                    std::env::set_var("OPENAI_BASE", base_url);
                }
            }
            tracing::debug!("ReactLlmClient: Set base URL for '{}': {}", provider, base_url);
        }
    }

    /// æµå¼è°ƒç”¨ LLMï¼Œæ¯ä¸ª token é€šè¿‡ emitter å‘é€
    pub async fn stream_completion(
        &self,
        system_prompt: Option<&str>,
        user_prompt: &str,
        iteration: u32,
    ) -> Result<String> {
        let provider = self.config.provider.to_lowercase();
        let model = &self.config.model;

        info!(
            "ReAct LLM stream request - Provider: {}, Model: {}, Iteration: {}",
            provider, model, iteration
        );
        
        // è®°å½• prompt åˆ°æ—¥å¿—
        log_prompts_react("ReactLlmClient", system_prompt, user_prompt);

        // è®¾ç½® rig åº“æ‰€éœ€çš„ç¯å¢ƒå˜é‡
        self.setup_env_vars();

        // æ„å»ºç”¨æˆ·æ¶ˆæ¯
        let user_message = Message::User {
            content: OneOrMany::one(UserContent::text(user_prompt.to_string())),
        };

        let preamble = system_prompt.unwrap_or("You are a helpful AI assistant.");
        let timeout = std::time::Duration::from_secs(self.config.timeout_secs);

        // æ ¹æ® provider åˆ›å»º agent å¹¶æ‰§è¡Œ
        let content = match provider.as_str() {
            "openai" | "lm studio" | "lmstudio" | "lm_studio" => {
                self.stream_with_openai(model, preamble, user_message, timeout).await?
            }
            "anthropic" => {
                self.stream_with_anthropic(model, preamble, user_message, timeout).await?
            }
            "gemini" | "google" => {
                self.stream_with_gemini(model, preamble, user_message, timeout).await?
            }
            "ollama" => {
                self.stream_with_ollama(model, preamble, user_message, timeout).await?
            }
            "deepseek" => {
                self.stream_with_deepseek(model, preamble, user_message, timeout).await?
            }
            "openrouter" => {
                self.stream_with_openrouter(model, preamble, user_message, timeout).await?
            }
            "xai" => {
                self.stream_with_xai(model, preamble, user_message, timeout).await?
            }
            "groq" => {
                self.stream_with_groq(model, preamble, user_message, timeout).await?
            }
            _ => {
                info!("Unknown provider '{}', trying OpenAI compatible mode", provider);
                self.stream_with_openai(model, preamble, user_message, timeout).await?
            }
        };

        info!(
            "ReactLlmClient: Response length: {} chars, Iteration: {}",
            content.len(), iteration
        );
        
        // è®°å½•å“åº”åˆ°æ—¥å¿—æ–‡ä»¶
        log_response_react("ReactLlmClient", &content);

        Ok(content)
    }

    async fn stream_with_openai(
        &self,
        model: &str,
        preamble: &str,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String> {
        use rig::providers::openai;
        let client = openai::Client::from_env();
        let agent = client.agent(model).preamble(preamble).build();
        self.execute_stream(agent, user_message, timeout).await
    }

    async fn stream_with_anthropic(
        &self,
        model: &str,
        preamble: &str,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String> {
        use rig::providers::anthropic;
        let client = anthropic::Client::from_env();
        let agent = client.agent(model).preamble(preamble).max_tokens(4096).build();
        self.execute_stream(agent, user_message, timeout).await
    }

    async fn stream_with_gemini(
        &self,
        model: &str,
        preamble: &str,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String> {
        use rig::providers::gemini;
        use rig::providers::gemini::completion::gemini_api_types::{AdditionalParameters, GenerationConfig};
        let client = gemini::Client::from_env();
        let gen_cfg = GenerationConfig::default();
        let cfg = AdditionalParameters::default().with_config(gen_cfg);
        let agent = client.agent(model)
            .preamble(preamble)
            .additional_params(serde_json::to_value(cfg).unwrap())
            .build();
        self.execute_stream(agent, user_message, timeout).await
    }

    async fn stream_with_ollama(
        &self,
        model: &str,
        preamble: &str,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String> {
        use rig::providers::ollama;
        let client = ollama::Client::from_env();
        let agent = client.agent(model).preamble(preamble).build();
        self.execute_stream(agent, user_message, timeout).await
    }

    async fn stream_with_deepseek(
        &self,
        model: &str,
        preamble: &str,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String> {
        use rig::providers::deepseek;
        
        // è·å– API Key
        let api_key = std::env::var("DEEPSEEK_API_KEY")
            .or_else(|_| std::env::var("OPENAI_API_KEY"))
            .map_err(|_| anyhow::anyhow!("DEEPSEEK_API_KEY not set"))?;
        
        // åˆ›å»ºå¸¦æœ‰æ­£ç¡® Content-Type çš„ HTTP å®¢æˆ·ç«¯ï¼ˆDeepSeek API è¦æ±‚ï¼‰
        let mut headers = reqwest::header::HeaderMap::new();
        headers.insert(
            reqwest::header::CONTENT_TYPE,
            reqwest::header::HeaderValue::from_static("application/json"),
        );
        
        let http_client = reqwest::Client::builder()
            .default_headers(headers)
            .build()
            .map_err(|e| anyhow::anyhow!("Failed to build HTTP client: {}", e))?;
        
        let client = deepseek::Client::<reqwest::Client>::builder()
            .api_key(api_key)
            .http_client(http_client)
            .build()
            .map_err(|e| anyhow::anyhow!("Failed to build DeepSeek client: {}", e))?;
        
        let agent = client.agent(model).preamble(preamble).build();
        self.execute_stream(agent, user_message, timeout).await
    }

    async fn stream_with_openrouter(
        &self,
        model: &str,
        preamble: &str,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String> {
        use rig::providers::openrouter;
        let client = openrouter::Client::from_env();
        let agent = client.agent(model).preamble(preamble).build();
        self.execute_stream(agent, user_message, timeout).await
    }

    async fn stream_with_xai(
        &self,
        model: &str,
        preamble: &str,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String> {
        use rig::providers::xai;
        let client = xai::Client::from_env();
        let agent = client.agent(model).preamble(preamble).build();
        self.execute_stream(agent, user_message, timeout).await
    }

    async fn stream_with_groq(
        &self,
        model: &str,
        preamble: &str,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String> {
        use rig::providers::groq;
        let client = groq::Client::from_env();
        let agent = client.agent(model).preamble(preamble).build();
        self.execute_stream(agent, user_message, timeout).await
    }

    async fn execute_stream<M>(
        &self,
        agent: rig::agent::Agent<M>,
        user_message: Message,
        timeout: std::time::Duration,
    ) -> Result<String>
    where
        M: rig::completion::CompletionModel + 'static,
        M::StreamingResponse: Clone + Unpin + rig::completion::GetTokenUsage,
    {
        // æµå¼è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶ï¼‰
        let stream_result = tokio::time::timeout(
            timeout,
            agent.stream_prompt(user_message).multi_turn(100),
        )
        .await;

        let mut stream_iter = match stream_result {
            Ok(iter) => iter,
            Err(_) => {
                error!(
                    "ReactLlmClient: Request timeout after {} seconds",
                    self.config.timeout_secs
                );
                return Err(anyhow!(
                    "ReAct LLM request timeout after {} seconds",
                    self.config.timeout_secs
                ));
            }
        };

        // å¤„ç†æµå¼å“åº”
        let mut content = String::new();
        while let Some(item) = stream_iter.next().await {
            match item {
                Ok(MultiTurnStreamItem::StreamAssistantItem(StreamedAssistantContent::Text(t))) => {
                    let piece = t.text;
                    if !piece.is_empty() {
                        content.push_str(&piece);
                        // é€šè¿‡ emitter å‘é€æ¯ä¸ª token
                        self.emitter.emit_content(&piece, false);
                    }
                }
                Ok(MultiTurnStreamItem::StreamAssistantItem(
                    StreamedAssistantContent::Reasoning(r),
                )) => {
                    let piece = r.reasoning.join("");
                    if !piece.is_empty() {
                        self.emitter.emit_thinking(&piece);
                    }
                }
                Ok(MultiTurnStreamItem::FinalResponse(_)) => {
                    debug!("ReactLlmClient: Stream completed");
                    break;
                }
                Ok(_) => { /* ignore other stream items */ }
                Err(e) => {
                    error!("ReactLlmClient: Stream error: {}", e);
                    return Err(anyhow!("ReAct LLM stream error: {}", e));
                }
            }
        }

        Ok(content)
    }
}

/// è®°å½• prompts åˆ° LLM æ—¥å¿—æ–‡ä»¶
fn log_prompts_react(client_name: &str, system_prompt: Option<&str>, user_prompt: &str) {
    write_llm_log_react(client_name, "REQUEST", system_prompt, user_prompt, None);
}

/// è®°å½• LLM å“åº”åˆ°æ—¥å¿—æ–‡ä»¶
fn log_response_react(client_name: &str, response: &str) {
    write_llm_log_react(client_name, "RESPONSE", None, "", Some(response));
}

/// å†™å…¥ LLM æ—¥å¿—åˆ°æ–‡ä»¶
fn write_llm_log_react(
    client_name: &str,
    log_type: &str,
    system_prompt: Option<&str>,
    user_prompt: &str,
    response: Option<&str>,
) {
    let timestamp = Utc::now().format("%Y-%m-%d %H:%M:%S%.3f UTC");
    
    let content = if let Some(resp) = response {
        // å®‰å…¨æˆªæ–­ï¼Œç¡®ä¿ä¸åœ¨ UTF-8 å­—ç¬¦ä¸­é—´åˆ‡æ–­
        let truncated = if resp.len() > 2000 {
            let mut end = 2000;
            while end > 0 && !resp.is_char_boundary(end) {
                end -= 1;
            }
            &resp[..end]
        } else {
            resp
        };
        format!(
            "Response ({} chars):\n{}\n",
            resp.len(),
            truncated
        )
    } else {
        format!(
            "\nUser Prompt:\n{}\n",
            // system_prompt.unwrap_or("(none)"),
            user_prompt
        )
    };
    
    let log_entry = format!(
        "\n{}\n[{}] [{}] [Client: {}]\n{}\n{}\n",
        "=".repeat(80), timestamp, log_type, client_name, "=".repeat(80), content
    );

    // ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    if let Err(e) = std::fs::create_dir_all("logs") {
        error!("Failed to create logs directory: {}", e);
        return;
    }

    // å†™å…¥ä¸“é—¨çš„ LLM è¯·æ±‚æ—¥å¿—æ–‡ä»¶
    let log_file_path = format!(
        "logs/llm-http-requests-{}.log",
        Utc::now().format("%Y-%m-%d")
    );

    match OpenOptions::new()
        .create(true)
        .append(true)
        .open(&log_file_path)
    {
        Ok(mut file) => {
            if let Err(e) = file.write_all(log_entry.as_bytes()) {
                error!("Failed to write to LLM log file {}: {}", log_file_path, e);
            } else {
                let _ = file.flush();
            }
        }
        Err(e) => {
            error!("Failed to open LLM log file {}: {}", log_file_path, e);
        }
    }
}
