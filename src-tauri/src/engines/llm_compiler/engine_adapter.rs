//! LLMCompilerå¼•æ“é€‚é…å™¨ - å®ç°ç»Ÿä¸€çš„ExecutionEngineæ¥å£

use super::executor::ParallelExecutorPool;
use super::joiner::IntelligentJoiner;
use super::planner::LlmCompilerPlanner;
use super::task_fetcher::TaskFetchingUnit;
use super::types::*;
use crate::agents::traits::*;
use crate::commands::agent_commands::WorkflowStepDetail;
use crate::models::prompt::{ArchitectureType, StageType};
use crate::services::ai::{AiService, AiServiceManager};
use crate::services::database::Database; // trait for DB methods
use crate::services::database::DatabaseService;
use crate::services::prompt_db::PromptRepository;
use crate::engines::memory::get_global_memory;
use crate::tools::ToolExecutionParams;
use crate::utils::ordered_message::{emit_message_chunk, ChunkType};
use anyhow::Result;
use async_trait::async_trait;
use std::collections::HashMap;
use std::sync::Arc;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

/// LLMCompilerå¼•æ“é€‚é…å™¨ - é›†æˆåŸæœ‰å¼•æ“é€»è¾‘
pub struct LlmCompilerEngine {
    engine_info: EngineInfo,
    // æ ¸å¿ƒç»„ä»¶
    planner: Option<Arc<LlmCompilerPlanner>>,
    task_fetcher: Option<Arc<TaskFetchingUnit>>,
    executor_pool: Option<Arc<ParallelExecutorPool>>,
    joiner: Option<Arc<tokio::sync::Mutex<IntelligentJoiner>>>,
    ai_service: Option<Arc<AiService>>,
    ai_service_manager: Option<Arc<AiServiceManager>>,  // âœ… æ·»åŠ AIæœåŠ¡ç®¡ç†å™¨ï¼Œç”¨äºè·å–å„é˜¶æ®µæ¨¡å‹
    config: LlmCompilerConfig,
    prompt_repo: Option<PromptRepository>,
    runtime_params: Option<std::collections::HashMap<String, serde_json::Value>>, // prompt_ids ç­‰
    app_handle: Option<tauri::AppHandle>,
    db_service: Option<Arc<DatabaseService>>,
}

impl LlmCompilerEngine {
    /// åˆ›å»ºæ–°çš„å¼•æ“é€‚é…å™¨
    pub async fn new() -> Result<Self> {
        let engine_info = EngineInfo {
            name: "LLMCompiler".to_string(),
            version: "1.0.0".to_string(),
            description: "DAG-based parallel execution architecture with intelligent task scheduling and dependency resolution".to_string(),
            supported_scenarios: vec![
                "Complex Multi-step Tasks".to_string(),
                "High Concurrency Processing".to_string(),
                "Large Scale Operations".to_string(),
                "Performance Critical Tasks".to_string(),
            ],
            performance_characteristics: PerformanceCharacteristics {
                token_efficiency: 90,
                execution_speed: 95,
                resource_usage: 80,
                concurrency_capability: 95,
                complexity_handling: 90,
            },
        };

        let config = LlmCompilerConfig::default();

        Ok(Self {
            engine_info,
            planner: None,
            task_fetcher: None,
            executor_pool: None,
            joiner: None,
            ai_service: None,
            ai_service_manager: None,
            config,
            prompt_repo: None,
            runtime_params: None,
            app_handle: None,
            db_service: None,
        })
    }

    /// ä½¿ç”¨å®Œæ•´ä¾èµ–åˆ›å»ºå¼•æ“é€‚é…å™¨
    pub async fn new_with_dependencies(
        ai_service_manager: Arc<AiServiceManager>,
        config: LlmCompilerConfig,
        db_service: Arc<DatabaseService>,
    ) -> Result<Self> {
        let engine_info = EngineInfo {
            name: "LLMCompiler".to_string(),
            version: "1.0.0".to_string(),
            description: "DAG-based parallel execution architecture with intelligent task scheduling and dependency resolution".to_string(),
            supported_scenarios: vec![
                "Complex Multi-step Tasks".to_string(),
                "High Concurrency Processing".to_string(),
                "Large Scale Operations".to_string(),
                "Performance Critical Tasks".to_string(),
            ],
            performance_characteristics: PerformanceCharacteristics {
                token_efficiency: 90,
                execution_speed: 95,
                resource_usage: 80,
                concurrency_capability: 95,
                complexity_handling: 90,
            },
        };

        // åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        let pool = db_service
            .get_pool()
            .map_err(|e| anyhow::anyhow!("DB pool error: {}", e))?;
        let tool_adapter = crate::tools::get_global_engine_adapter()
            .map_err(|e| anyhow::anyhow!("è·å–å…¨å±€å·¥å…·é€‚é…å™¨å¤±è´¥: {}", e))?;

        // âœ… è·å–å„é˜¶æ®µçš„AIæœåŠ¡
        // Planneré˜¶æ®µä½¿ç”¨Planningæ¨¡å‹
        let planner_service = Self::get_ai_service_for_stage(&ai_service_manager, crate::services::ai::SchedulerStage::Planning).await?;
        // Joineré˜¶æ®µä½¿ç”¨Evaluationæ¨¡å‹
        let joiner_service = Self::get_ai_service_for_stage(&ai_service_manager, crate::services::ai::SchedulerStage::Evaluation).await?;
        // é»˜è®¤AIæœåŠ¡ä½œä¸ºåå¤‡
        let default_service = Self::get_ai_service_from_manager(&ai_service_manager)?;

        info!("LLMCompiler: Planner using model: {}/{}", planner_service.get_config().provider, planner_service.get_config().model);
        info!("LLMCompiler: Joiner using model: {}/{}", joiner_service.get_config().provider, joiner_service.get_config().model);

        let planner = Some(Arc::new(LlmCompilerPlanner::new(
            planner_service,
            tool_adapter.clone(),
            config.clone(),
            Some(PromptRepository::new(pool.clone())),
        )));

        let task_fetcher = Some(Arc::new(TaskFetchingUnit::new(config.clone())));

        let executor_pool = Some(Arc::new(ParallelExecutorPool::new(
            tool_adapter.clone(),
            config.clone(),
        )));

        let joiner = Some(Arc::new(tokio::sync::Mutex::new(IntelligentJoiner::new(
            (*joiner_service).clone(),
            config.clone(),
            Some(PromptRepository::new(pool.clone())),
        ))));

        Ok(Self {
            engine_info,
            planner,
            task_fetcher,
            executor_pool,
            joiner,
            ai_service: Some(default_service),
            ai_service_manager: Some(ai_service_manager),  // âœ… ä¿å­˜AIæœåŠ¡ç®¡ç†å™¨
            config,
            prompt_repo: Some(PromptRepository::new(pool.clone())),
            runtime_params: None,
            app_handle: None,
            db_service: Some(db_service),
        })
    }

    pub fn set_runtime_params(
        &mut self,
        params: std::collections::HashMap<String, serde_json::Value>,
    ) {
        self.runtime_params = Some(params.clone());
        // é€ä¼ åˆ°å„ç»„ä»¶ï¼ˆplanner/joiner/executorï¼‰
        if let Some(_planner) = &self.planner {
            // Plannerç›®å‰æ˜¯Arcä¸”æ²¡æœ‰å†…éƒ¨å¯å˜æ€§æ¥å£ï¼Œè¿™é‡Œé€šè¿‡ä»»åŠ¡æ‰§è¡Œæ—¶è¯»å–engineçš„runtime_params
            // å¦‚æœåç»­Plannerå¢åŠ set_runtime_paramsï¼Œåˆ™å¯åœ¨æ­¤å¤„ä¸‹å‘
        }
        if let Some(_joiner) = &self.joiner {
            // Joinerå†…éƒ¨ä¼šåœ¨åˆ†ææ—¶ä»å¼•æ“è¯»å–ä¸Šä¸‹æ–‡ï¼›å¦‚éœ€å¯æ‰©å±•ä¼ å…¥
        }
        // å°†å‚æ•°ä¼ é€’ç»™æ‰§è¡Œå™¨æ± 
        if let Some(executor_pool) = &self.executor_pool {
            let exec_clone = executor_pool.clone();
            let params_clone = params.clone();
            tokio::spawn(async move {
                exec_clone.set_runtime_params(params_clone).await;
            });
        }
    }

    /// è®¾ç½®åº”ç”¨å¥æŸ„ç”¨äºå‘é€æ¶ˆæ¯
    pub fn set_app_handle(&mut self, app_handle: tauri::AppHandle) {
        self.app_handle = Some(app_handle);
    }

    /// å‘é€é”™è¯¯æ¶ˆæ¯åˆ°å‰ç«¯
    fn emit_error(
        &self,
        execution_id: &str,
        message_id: &str,
        conversation_id: Option<&str>,
        error_msg: &str,
    ) {
        if let Some(ref app_handle) = self.app_handle {
            let error_message = format!(
                "LLMCompilerå¼•æ“æ‰§è¡Œå¤±è´¥: {}\n\nå¦‚éœ€å¸®åŠ©ï¼Œè¯·æ£€æŸ¥æ‰§è¡Œé…ç½®æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
                error_msg
            );
            emit_message_chunk(
                app_handle,
                execution_id,
                message_id,
                conversation_id,
                ChunkType::Error,
                &error_message,
                true,
                Some("llm_compiler"),
                None,
            );
        }
    }

    /// âœ… æ ¹æ®è°ƒåº¦å™¨é˜¶æ®µè·å–å¯¹åº”çš„AIæœåŠ¡
    async fn get_ai_service_for_stage(
        ai_service_manager: &Arc<AiServiceManager>,
        stage: crate::services::ai::SchedulerStage,
    ) -> Result<Arc<AiService>> {
        // é¦–å…ˆå°è¯•ä»è°ƒåº¦å™¨é…ç½®è·å–è¯¥é˜¶æ®µçš„æ¨¡å‹
        match ai_service_manager.get_ai_config_for_stage(stage.clone()).await {
            Ok(Some(config)) => {
                info!(
                    "LLMCompiler: Using scheduler config for stage {:?} -> provider: {}, model: {}",
                    stage, config.provider, config.model
                );
                
                // æ ¹æ®providerå’Œmodelåˆ›å»ºAIæœåŠ¡key
                // æ ¼å¼ï¼šprovider::model
                let service_key = format!("{}::{}", config.provider, config.model);
                if let Some(service) = ai_service_manager.get_service(&service_key) {
                    return Ok(Arc::new(service));
                }
                
                // å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•åªç”¨providerä½œä¸ºkeyï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
                if let Some(service) = ai_service_manager.get_service(&config.provider) {
                    return Ok(Arc::new(service));
                }
                
                // âœ… å°è¯•å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…ï¼ˆéå†æ‰€æœ‰å¯ç”¨æœåŠ¡ï¼‰
                let provider_lower = config.provider.to_lowercase();
                let all_services = ai_service_manager.list_services();
                for service_name in &all_services {
                    if service_name.to_lowercase() == provider_lower {
                        if let Some(service) = ai_service_manager.get_service(service_name) {
                            info!(
                                "LLMCompiler: Found service '{}' via case-insensitive match for provider '{}'",
                                service_name, config.provider
                            );
                            return Ok(Arc::new(service));
                        }
                    }
                }
                
                warn!(
                    "LLMCompiler: No service found for stage {:?} (provider: {}, model: {}), falling back to default",
                    stage, config.provider, config.model
                );
            }
            Ok(None) => {
                info!("LLMCompiler: No scheduler config for stage {:?}, using default service", stage);
            }
            Err(e) => {
                warn!(
                    "LLMCompiler: Failed to get scheduler config for stage {:?}: {}, using default service",
                    stage, e
                );
            }
        }
        
        // å¦‚æœæ²¡æœ‰é…ç½®æˆ–è·å–å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æœåŠ¡
        Self::get_ai_service_from_manager(ai_service_manager)
    }

    /// ä»AIæœåŠ¡ç®¡ç†å™¨è·å–é»˜è®¤AIæœåŠ¡
    fn get_ai_service_from_manager(
        ai_service_manager: &Arc<AiServiceManager>,
    ) -> Result<Arc<AiService>> {
        // åˆ—å‡ºæ‰€æœ‰å¯ç”¨æœåŠ¡ç”¨äºè°ƒè¯•
        let all_services = ai_service_manager.list_services();
        info!("LLMCompiler: Available AI services: {:?}", all_services);
        
        // é¦–å…ˆå°è¯•è·å–"default"æœåŠ¡
        if let Some(service) = ai_service_manager.get_service("default") {
            let config = service.get_config();
            info!(
                "LLMCompiler: Using 'default' AI service -> provider: {}, model: {}",
                config.provider, config.model
            );
            return Ok(Arc::new(service));
        }

        // å¦‚æœæ²¡æœ‰defaultï¼Œå°è¯•æŒ‰ä¼˜å…ˆçº§è·å–
        let preferred_providers = vec![
            "deepseek",
            "openai",
            "anthropic",
            "gemini",
            "groq",
            "modelscope",
            "openrouter",
        ];
        for provider in &preferred_providers {
            if let Some(service) = ai_service_manager.get_service(provider) {
                let config = service.get_config();
                info!(
                    "LLMCompiler: Using preferred provider '{}' -> model: {}",
                    config.provider, config.model
                );
                return Ok(Arc::new(service));
            }
        }

        // è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„æœåŠ¡
        let services = ai_service_manager.list_services();
        if let Some(first_service_name) = services.first() {
            if let Some(service) = ai_service_manager.get_service(first_service_name) {
                let config = service.get_config();
                warn!(
                    "LLMCompiler: No preferred provider found, using first available service '{}' -> provider: {}, model: {}",
                    first_service_name, config.provider, config.model
                );
                warn!("Recommended: Configure a preferred AI provider in Settings > AI Configuration");
                return Ok(Arc::new(service));
            }
        }

        error!("LLMCompiler: No AI service available. Please configure at least one AI provider.");
        Err(anyhow::anyhow!(
            "No AI service available in AiServiceManager. Please configure an AI provider in Settings."
        ))
    }
}

#[async_trait]
impl ExecutionEngine for LlmCompilerEngine {
    fn get_engine_info(&self) -> &EngineInfo {
        &self.engine_info
    }

    fn supports_task(&self, task: &AgentTask) -> bool {
        // LLMCompileré€‚åˆå¤æ‚å¤šæ­¥éª¤ã€é«˜å¹¶å‘éœ€æ±‚çš„ä»»åŠ¡
        if task.description.to_lowercase().contains("complex")
            || task.description.to_lowercase().contains("parallel")
            || task.description.to_lowercase().contains("concurrent")
            || task.description.to_lowercase().contains("large")
            || task.description.to_lowercase().contains("performance")
        {
            return true;
        }

        // æ£€æŸ¥ä¼˜å…ˆçº§æ˜¯å¦ä¸ºé«˜æˆ–ç´§æ€¥
        matches!(task.priority, TaskPriority::High | TaskPriority::Critical)
    }

    async fn create_plan(&self, task: &AgentTask) -> Result<ExecutionPlan> {
        // åˆ›å»ºLLMCompileré£æ ¼çš„DAGæ‰§è¡Œè®¡åˆ’
        let steps = vec![
            ExecutionStep {
                id: "dag_planner".to_string(),
                name: "Create Task DAG".to_string(),
                description:
                    "Generate DAG with task dependencies and parallel execution opportunities"
                        .to_string(),
                step_type: StepType::LlmCall,
                dependencies: vec![],
                parameters: [(
                    "dag_mode".to_string(),
                    serde_json::Value::String("parallel".to_string()),
                )]
                .into_iter()
                .collect(),
            },
            ExecutionStep {
                id: "task_fetcher".to_string(),
                name: "Fetch Ready Tasks".to_string(),
                description: "Identify and fetch tasks ready for execution".to_string(),
                step_type: StepType::DataProcessing,
                dependencies: vec!["dag_planner".to_string()],
                parameters: HashMap::new(),
            },
            ExecutionStep {
                id: "parallel_executor".to_string(),
                name: "Execute Tasks in Parallel".to_string(),
                description: "Execute multiple tasks concurrently based on DAG".to_string(),
                step_type: StepType::Parallel,
                dependencies: vec!["task_fetcher".to_string()],
                parameters: [(
                    "max_concurrent".to_string(),
                    serde_json::Value::Number(serde_json::Number::from(10)),
                )]
                .into_iter()
                .collect(),
            },
            ExecutionStep {
                id: "dependency_resolver".to_string(),
                name: "Resolve Dependencies".to_string(),
                description: "Manage task dependencies and execution flow".to_string(),
                step_type: StepType::DataProcessing,
                dependencies: vec!["parallel_executor".to_string()],
                parameters: HashMap::new(),
            },
            ExecutionStep {
                id: "joiner".to_string(),
                name: "Join Results".to_string(),
                description: "Aggregate and join all task results".to_string(),
                step_type: StepType::LlmCall,
                dependencies: vec!["dependency_resolver".to_string()],
                parameters: [(
                    "join_strategy".to_string(),
                    serde_json::Value::String("intelligent".to_string()),
                )]
                .into_iter()
                .collect(),
            },
        ];

        let plan = ExecutionPlan {
            id: uuid::Uuid::new_v4().to_string(),
            name: format!("LLMCompiler: {}", task.description),
            steps,
            estimated_duration: 120, // 2åˆ†é’Ÿï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰
            resource_requirements: ResourceRequirements {
                cpu_cores: Some(8),
                memory_mb: Some(2048),
                network_concurrency: Some(50),
                disk_space_mb: Some(200),
            },
        };

        Ok(plan)
    }

    async fn execute_plan(&self, plan: &ExecutionPlan) -> Result<AgentExecutionResult> {
        let _start_time = std::time::Instant::now();

        // å¦‚æœæœ‰å®Œæ•´çš„LLMCompilerç»„ä»¶ï¼Œä½¿ç”¨åŸæœ‰çš„å·¥ä½œæµæ‰§è¡Œé€»è¾‘
        if let (
            Some(_planner),
            Some(_task_fetcher),
            Some(_executor_pool),
            Some(_joiner),
            Some(_ai_service),
        ) = (
            &self.planner,
            &self.task_fetcher,
            &self.executor_pool,
            &self.joiner,
            &self.ai_service,
        ) {
            match self.execute_llm_compiler_workflow(plan).await {
                Ok(result) => return Ok(result),
                Err(e) => {
                    return Err(e.into());
                }
            }
        } else {
            return Err(anyhow::anyhow!("LLMCompilerEngine execute_plan error"));
        }
    }

    async fn get_progress(&self, _session_id: &str) -> Result<ExecutionProgress> {
        // æ¨¡æ‹Ÿè¿›åº¦æŸ¥è¯¢
        Ok(ExecutionProgress {
            total_steps: 5,
            completed_steps: 4,
            current_step: Some("Join Results".to_string()),
            progress_percentage: 80.0,
            estimated_remaining_seconds: Some(30),
        })
    }

    async fn cancel_execution(&self, session_id: &str) -> Result<()> {
        log::info!(
            "Cancelling LLMCompiler execution for session: {}",
            session_id
        );
        // ç›´æ¥å®ç°å–æ¶ˆé€»è¾‘
        if let Some(task_fetcher) = &self.task_fetcher {
            task_fetcher.cancel_pending_tasks().await?;
        }
        Ok(())
    }
}

impl LlmCompilerEngine {
    /// æ‰§è¡ŒåŸæœ‰çš„LLMCompilerå·¥ä½œæµé€»è¾‘
    async fn execute_llm_compiler_workflow(
        &self,
        _plan: &ExecutionPlan,
    ) -> Result<AgentExecutionResult> {
        let workflow_start = std::time::Instant::now();

        // âœ… ä»runtime_paramsä¸­è·å–ç”¨æˆ·çš„å®é™…ä»»åŠ¡æè¿°å’Œå‰ç«¯æ¶ˆæ¯ID
        let context = self.runtime_params.clone().unwrap_or_default();
        let user_query = context
            .get("task_description")
            .and_then(|v| v.as_str())
            .unwrap_or("LLMCompiler workflow execution");

        // âœ… æå–conversation_idã€message_idã€execution_idï¼ˆç”¨äºç»Ÿä¸€æ¶ˆæ¯æ¨é€ï¼‰
        let conversation_id = context
            .get("conversation_id")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());
        let message_id = context
            .get("message_id")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());
        let execution_id = context
            .get("execution_id")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());

        info!("å¼€å§‹æ‰§è¡ŒLLMCompilerå·¥ä½œæµ: {}", user_query);
        info!("LLMCompiler: conversation_id={:?}, message_id={:?}, execution_id={:?}", 
            conversation_id, message_id, execution_id);

        let mut execution_summary = ExecutionSummary {
            total_tasks: 0,
            successful_tasks: 0,
            failed_tasks: 0,
            total_duration_ms: 0,
            replanning_count: 0,
            key_findings: Vec::new(),
            efficiency_metrics: EfficiencyMetrics {
                average_parallelism: 0.0,
                resource_utilization: 0.0,
                task_success_rate: 0.0,
                average_task_duration_ms: 0.0,
            },
        };

        let mut all_results: Vec<TaskExecutionResult> = Vec::new();

        // ä¸»æ‰§è¡Œå¾ªç¯
        for round in 1..=self.config.max_iterations {
            info!("å¼€å§‹æ‰§è¡Œè½®æ¬¡: {}/{}", round, self.config.max_iterations);

            if round > 1 {
                execution_summary.replanning_count = round - 1;
            }

            // 1. è§„åˆ’é˜¶æ®µ
            if let Some(planner) = &self.planner {
                info!("å¼€å§‹ç”ŸæˆDAGæ‰§è¡Œè®¡åˆ’...");
                
                // å‘é€Planningé˜¶æ®µå¼€å§‹æ¶ˆæ¯
                if let Some(app) = &self.app_handle {
                    crate::utils::ordered_message::emit_thinking_chunk(
                        app,
                        execution_id.as_deref().unwrap_or("unknown"),
                        message_id.as_deref().unwrap_or("unknown"),
                        conversation_id.as_deref(),
                        "å¼€å§‹ç”ŸæˆDAGæ‰§è¡Œè®¡åˆ’...",
                        Some("llm_compiler_planning"),
                    );
                }
                
                let execution_plan = match planner.generate_dag_plan(user_query, &context).await {
                    Ok(plan) => {
                        info!("âœ“ DAGè§„åˆ’æˆåŠŸ: {} ä¸ªä»»åŠ¡èŠ‚ç‚¹", plan.nodes.len());
                        
                        // å‘é€è®¡åˆ’ä¿¡æ¯åˆ°å‰ç«¯
                        if let Some(app) = &self.app_handle {
                            let plan_info = format!(
                                "DAGè§„åˆ’æˆåŠŸï¼Œå…±{}ä¸ªä»»åŠ¡èŠ‚ç‚¹",
                                plan.nodes.len()
                            );
                            crate::utils::ordered_message::emit_plan_info_chunk(
                                app,
                                execution_id.as_deref().unwrap_or("unknown"),
                                message_id.as_deref().unwrap_or("unknown"),
                                conversation_id.as_deref(),
                                &plan_info,
                                Some("llm_compiler_planning"),
                                None,
                            );
                        }
                        
                        plan
                    }
                    Err(e) => {
                        error!("âœ— DAGè§„åˆ’å¤±è´¥: {}", e);
                        
                        // å‘é€é”™è¯¯æ¶ˆæ¯åˆ°å‰ç«¯
                        if let Some(app) = &self.app_handle {
                            let error_msg = format!("DAGè§„åˆ’å¤±è´¥: {}", e);
                            crate::utils::ordered_message::emit_error_chunk(
                                app,
                                execution_id.as_deref().unwrap_or("unknown"),
                                message_id.as_deref().unwrap_or("unknown"),
                                conversation_id.as_deref(),
                                &error_msg,
                                Some("llm_compiler_planning"),
                                None,
                            );
                        }
                        
                        return Err(anyhow::anyhow!(
                            "LLMCompiler planning phase failed: {}. This may be due to LLM service configuration issues or network problems.",
                            e
                        ));
                    }
                };

                // 2. ä»»åŠ¡è°ƒåº¦é˜¶æ®µ
                if let Some(task_fetcher) = &self.task_fetcher {
                    task_fetcher.initialize_plan(&execution_plan).await?;

                    // 3. å¹¶è¡Œæ‰§è¡Œé˜¶æ®µ
                    let round_results = self.execute_parallel_round().await?;

                    if round_results.is_empty() {
                        warn!("è½®æ¬¡ {} æ²¡æœ‰æ‰§è¡Œä»»ä½•ä»»åŠ¡", round);
                        break;
                    }

                    // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    let completed_count = round_results
                        .iter()
                        .filter(|r| r.status == TaskStatus::Completed)
                        .count();
                    let failed_count = round_results
                        .iter()
                        .filter(|r| r.status == TaskStatus::Failed)
                        .count();

                    execution_summary.successful_tasks += completed_count;
                    execution_summary.failed_tasks += failed_count;
                    execution_summary.total_tasks += round_results.len();

                    let round_duration: u64 = round_results.iter().map(|r| r.duration_ms).sum();
                    execution_summary.total_duration_ms += round_duration;

                    all_results.extend(round_results.clone());

                    info!(
                        "è½®æ¬¡ {} å®Œæˆ: æˆåŠŸ {} ä¸ªä»»åŠ¡, å¤±è´¥ {} ä¸ªä»»åŠ¡, è€—æ—¶ {}ms",
                        round, completed_count, failed_count, round_duration
                    );

                // 4. æ™ºèƒ½å†³ç­–é˜¶æ®µ
                if let Some(joiner) = &self.joiner {
                    info!("å¼€å§‹ Joiner æ™ºèƒ½å†³ç­–é˜¶æ®µ...");
                    
                    // å‘é€Joineré˜¶æ®µå¼€å§‹æ¶ˆæ¯
                    if let Some(app) = &self.app_handle {
                        crate::utils::ordered_message::emit_thinking_chunk(
                            app,
                            execution_id.as_deref().unwrap_or("unknown"),
                            message_id.as_deref().unwrap_or("unknown"),
                            conversation_id.as_deref(),
                            "å¼€å§‹æ™ºèƒ½å†³ç­–åˆ†æ...",
                            Some("llm_compiler_joiner"),
                        );
                    }
                    
                    let mut joiner_guard = joiner.lock().await;
                    // âœ… è®¾ç½®æ¶ˆæ¯IDï¼Œç¡®ä¿Joinerçš„AIè°ƒç”¨ä½¿ç”¨æ­£ç¡®çš„ID
                    joiner_guard.set_message_ids(conversation_id.clone(), message_id.clone());
                    let decision = match joiner_guard
                        .analyze_and_decide(user_query, &execution_plan, &round_results, round)
                        .await
                    {
                        Ok(d) => {
                            debug!("Joiner å†³ç­–æˆåŠŸ");
                            d
                        }
                        Err(e) => {
                            warn!("Joiner å†³ç­–å¤±è´¥: {}", e);
                            
                            // å‘é€é”™è¯¯æ¶ˆæ¯åˆ°å‰ç«¯
                            if let Some(app) = &self.app_handle {
                                let error_msg = format!("Joinerå†³ç­–å¤±è´¥: {}", e);
                                crate::utils::ordered_message::emit_error_chunk(
                                    app,
                                    execution_id.as_deref().unwrap_or("unknown"),
                                    message_id.as_deref().unwrap_or("unknown"),
                                    conversation_id.as_deref(),
                                    &error_msg,
                                    Some("llm_compiler_joiner"),
                                    None,
                                );
                            }
                            
                            // Joiner å¤±è´¥ä¸åº”å¯¼è‡´æ•´ä¸ªæµç¨‹ä¸­æ–­ï¼Œè®°å½•è­¦å‘Šå¹¶ç»§ç»­
                            warn!("Joiner decision failed, continuing with available results");
                            JoinerDecision::Complete {
                                response: format!("Task execution completed with {} successful and {} failed tasks", 
                                    execution_summary.successful_tasks, 
                                    execution_summary.failed_tasks),
                                confidence: 0.7,
                                summary: execution_summary.clone(),
                            }
                        }
                    };

                    match &decision {
                        JoinerDecision::Complete { response, .. } => {
                            execution_summary
                                .key_findings
                                .push(format!("å†³ç­–: å®Œæˆæ‰§è¡Œ - {}", response));
                            info!("Joinerå†³å®šå®Œæˆæ‰§è¡Œ: {}", response);
                            
                            // å‘é€å†³ç­–ç»“æœåˆ°å‰ç«¯
                            if let Some(app) = &self.app_handle {
                                let decision_msg = format!("âœ“ å†³ç­–: å®Œæˆæ‰§è¡Œ\n{}", response);
                                crate::utils::ordered_message::emit_meta_chunk(
                                    app,
                                    execution_id.as_deref().unwrap_or("unknown"),
                                    message_id.as_deref().unwrap_or("unknown"),
                                    conversation_id.as_deref(),
                                    &decision_msg,
                                    None,
                                );
                            }
                            
                            break;
                        }
                        JoinerDecision::Continue { feedback, .. } => {
                            execution_summary
                                .key_findings
                                .push(format!("å†³ç­–: ç»§ç»­æ‰§è¡Œ - {}", feedback));
                            info!("Joinerå†³å®šç»§ç»­æ‰§è¡Œ: {}", feedback);
                            
                            // å‘é€å†³ç­–ç»“æœåˆ°å‰ç«¯
                            if let Some(app) = &self.app_handle {
                                let decision_msg = format!("â†’ å†³ç­–: ç»§ç»­æ‰§è¡Œ\n{}", feedback);
                                crate::utils::ordered_message::emit_meta_chunk(
                                    app,
                                    execution_id.as_deref().unwrap_or("unknown"),
                                    message_id.as_deref().unwrap_or("unknown"),
                                    conversation_id.as_deref(),
                                    &decision_msg,
                                    None,
                                );
                            }
                        }
                    }
                    }

                    // æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡
                    if !task_fetcher.has_pending_tasks().await {
                        info!("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œç»“æŸæ‰§è¡Œ");
                        break;
                    }
                }
            }
        }

        let total_duration = workflow_start.elapsed();
        execution_summary.total_duration_ms = total_duration.as_millis() as u64;

        // è®¡ç®—æœ€ç»ˆæ•ˆç‡æŒ‡æ ‡
        execution_summary.efficiency_metrics =
            self.calculate_efficiency_metrics(&execution_summary);

        info!(
            "å·¥ä½œæµæ‰§è¡Œå®Œæˆ: {} ä¸ªä»»åŠ¡, æˆåŠŸ {}, å¤±è´¥ {}, è€—æ—¶ {}ms",
            execution_summary.total_tasks,
            execution_summary.successful_tasks,
            execution_summary.failed_tasks,
            execution_summary.total_duration_ms
        );

        // ç”Ÿæˆæœ€ç»ˆç»“æœï¼ˆä¼ é€’æ­£ç¡®çš„conversation_idå’Œmessage_idï¼‰
        let final_response = self
            .generate_final_response(
                user_query, 
                &all_results, 
                &execution_summary,
                conversation_id.as_deref(),
                message_id.as_deref()
            )
            .await?;

        // è½¬æ¢ä¸ºAgentExecutionResult
        Ok(AgentExecutionResult {
            id: uuid::Uuid::new_v4().to_string(),
            success: true,
            data: Some(serde_json::json!({
                "engine": "llm_compiler",
                "response": final_response,
                "execution_summary": execution_summary,
                "task_results": all_results,
                "efficiency_metrics": execution_summary.efficiency_metrics
            })),
            error: None,
            execution_time_ms: execution_summary.total_duration_ms,
            resources_used: [
                (
                    "total_tasks".to_string(),
                    execution_summary.total_tasks as f64,
                ),
                (
                    "successful_tasks".to_string(),
                    execution_summary.successful_tasks as f64,
                ),
                (
                    "failed_tasks".to_string(),
                    execution_summary.failed_tasks as f64,
                ),
                (
                    "parallelism".to_string(),
                    execution_summary.efficiency_metrics.average_parallelism as f64,
                ),
            ]
            .into_iter()
            .collect(),
            artifacts: vec![ExecutionArtifact {
                artifact_type: ArtifactType::AnalysisResult,
                name: "llm_compiler_workflow_result".to_string(),
                data: serde_json::json!({
                    "execution_summary": execution_summary,
                    "task_results": all_results,
                    "final_response": final_response
                }),
                created_at: chrono::Utc::now(),
            }],
        })
    }

    /// æ‰§è¡Œä¸€è½®å¹¶è¡Œä»»åŠ¡
    async fn execute_parallel_round(&self) -> Result<Vec<TaskExecutionResult>> {
        let mut results = Vec::new();

        if let (Some(task_fetcher), Some(executor_pool)) = (&self.task_fetcher, &self.executor_pool)
        {
            // è·å–æ‰€æœ‰å°±ç»ªçš„ä»»åŠ¡
            let ready_tasks = task_fetcher
                .fetch_ready_tasks(self.config.max_concurrency)
                .await;

            if ready_tasks.is_empty() {
                return Ok(results);
            }

            info!("å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {} ä¸ªå°±ç»ªä»»åŠ¡", ready_tasks.len());

            // æ ‡è®°ä»»åŠ¡ä¸ºæ‰§è¡Œä¸­å¹¶å¯åŠ¨æ‰§è¡Œ
            let mut handles = Vec::new();
            for task in ready_tasks {
                let task_id = task.id.clone();
                let executor = executor_pool.clone();
                let handle = tokio::spawn(async move { executor.execute_task(task).await });

                // æ ‡è®°ä»»åŠ¡å¼€å§‹æ‰§è¡Œ
                task_fetcher
                    .mark_task_executing(task_id, handle.abort_handle())
                    .await;
                handles.push(handle);
            }

            // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for handle in handles {
                match handle.await {
                    Ok(result) => {
                        // âœ… å‘é€å·¥å…·æ‰§è¡Œç»“æœåˆ°å‰ç«¯ï¼ˆå‚è€ƒReactæ¶æ„ï¼‰
                        if let Some(app) = &self.app_handle {
                            if let Some(params) = &self.runtime_params {
                                let conversation_id = params.get("conversation_id").and_then(|v| v.as_str());
                                let message_id = params.get("message_id").and_then(|v| v.as_str());
                                let execution_id = params.get("execution_id").and_then(|v| v.as_str());
                                
                                // æ„é€ å·¥å…·ç»“æœå†…å®¹
                                let tool_result_content = if result.status == TaskStatus::Completed {
                                    serde_json::to_string(&result.outputs).unwrap_or_default()
                                } else {
                                    serde_json::json!({
                                        "error": result.error.clone().unwrap_or_else(|| "Unknown error".to_string()),
                                        "success": false,
                                        "task_id": result.task_id
                                    }).to_string()
                                };
                                
                                emit_message_chunk(
                                    app,
                                    execution_id.unwrap_or(&result.task_id),
                                    message_id.unwrap_or(&result.task_id),
                                    conversation_id,
                                    ChunkType::ToolResult,
                                    &tool_result_content,
                                    false,
                                    Some("llm_compiler"),
                                    None,  // å·¥å…·åç§°å¯ä»¥ä»resultä¸­æå–ï¼Œä½†è¿™é‡Œæš‚æ—¶ä¸ºNone
                                );
                                
                                info!("ğŸ“¤ LLMCompiler: Tool result sent to frontend: task={}, status={:?}", 
                                    result.task_id, result.status);
                            }
                        }
                        
                        // æ›´æ–°ä»»åŠ¡çŠ¶æ€
                        task_fetcher.complete_task(result.clone()).await?;
                        // è½åº“å•ä¸ªä»»åŠ¡ä½œä¸ºæ­¥éª¤
                        if let (Some(db), Some(params)) = (&self.db_service, &self.runtime_params) {
                            if let Some(session_id) =
                                params.get("execution_id").and_then(|v| v.as_str())
                            {
                                let step_name = result.task_id.clone();
                                let status = match result.status {
                                    TaskStatus::Completed => "Completed",
                                    TaskStatus::Failed => "Failed",
                                    _ => "Running",
                                };
                                let _ = db
                                    .save_agent_execution_step(
                                        session_id,
                                        &WorkflowStepDetail {
                                            step_id: format!("step_{}", results.len() + 1),
                                            step_name,
                                            status: status.to_string(),
                                            started_at: None,
                                            completed_at: None,
                                            duration_ms: result.duration_ms,
                                            result_data: Some(serde_json::json!(result.outputs)),
                                            error: result.error.clone(),
                                            retry_count: 0,
                                            dependencies: vec![],
                                            tool_result: None,
                                        },
                                    )
                                    .await;
                            }
                        }
                        results.push(result);
                    }
                    Err(e) => {
                        error!("ä»»åŠ¡æ‰§è¡Œå¥æŸ„é”™è¯¯: {}", e);
                    }
                }
            }
        }

        Ok(results)
    }

    /// ç”Ÿæˆæœ€ç»ˆå“åº”
    async fn generate_final_response(
        &self,
        user_query: &str,
        task_results: &[TaskExecutionResult],
        execution_summary: &ExecutionSummary,
        conversation_id: Option<&str>,
        message_id: Option<&str>,
    ) -> Result<String> {
        // æ”¶é›†æ‰€æœ‰æˆåŠŸä»»åŠ¡çš„è¾“å‡º
        let successful_outputs: Vec<&std::collections::HashMap<String, serde_json::Value>> =
            task_results
                .iter()
                .filter(|r| r.status == TaskStatus::Completed)
                .map(|r| &r.outputs)
                .collect();

        if successful_outputs.is_empty() {
            return Ok("æŠ±æ­‰ï¼Œæ²¡æœ‰æˆåŠŸæ‰§è¡Œä»»ä½•ä»»åŠ¡ï¼Œæ— æ³•æä¾›æœ‰æ•ˆç»“æœã€‚".to_string());
        }

        // æ„å»ºå“åº”ç”Ÿæˆæç¤º
        let mut response_prompt = if let Some(repo) = &self.prompt_repo {
            // ä¼˜å…ˆæŒ‰ prompt_ids.executor è¦†ç›–
            if let Some(rp) = &self.runtime_params {
                if let Some(pid) = rp
                    .get("prompt_ids")
                    .and_then(|v| v.get("executor"))
                    .and_then(|v| v.as_i64())
                {
                    if let Ok(Some(dynamic)) = repo.get_template(pid).await {
                        dynamic.content
                    } else if let Ok(Some(dynamic)) = repo
                        .get_active_prompt(ArchitectureType::LLMCompiler, StageType::Execution)
                        .await
                    {
                        dynamic
                    } else {
                        self.build_default_response_prompt(
                            user_query,
                            &successful_outputs,
                            execution_summary,
                        )
                    }
                } else if let Ok(Some(dynamic)) = repo
                    .get_active_prompt(ArchitectureType::LLMCompiler, StageType::Execution)
                    .await
                {
                    dynamic
                } else {
                    self.build_default_response_prompt(
                        user_query,
                        &successful_outputs,
                        execution_summary,
                    )
                }
            } else if let Ok(Some(dynamic)) = repo
                .get_active_prompt(ArchitectureType::LLMCompiler, StageType::Execution)
                .await
            {
                dynamic
            } else {
                self.build_default_response_prompt(
                    user_query,
                    &successful_outputs,
                    execution_summary,
                )
            }
        } else {
            self.build_default_response_prompt(user_query, &successful_outputs, execution_summary)
        };

        // é›†æˆè§’è‰²æç¤ºè¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if let Some(rp) = &self.runtime_params {
            if let Some(role_prompt) = rp.get("role_prompt").and_then(|v| v.as_str()) {
                if !role_prompt.trim().is_empty() {
                    response_prompt = if response_prompt.trim().is_empty() {
                        role_prompt.to_string()
                    } else {
                        format!("{}\n\n{}", role_prompt, response_prompt)
                    };
                    log::info!("LLM-Compiler joiner: integrated role prompt for final response");
                }
            }
        }

        // RAG augmentation for final response prompt
        log::debug!("RAG augmentation skipped in llm_compiler engine_adapter");

        // âœ… è°ƒç”¨AIç”Ÿæˆæœ€ç»ˆå“åº”ï¼Œä½¿ç”¨Executioné˜¶æ®µçš„æ¨¡å‹
        let execution_service = if let Some(ai_service_manager) = &self.ai_service_manager {
            match Self::get_ai_service_for_stage(ai_service_manager, crate::services::ai::SchedulerStage::Execution).await {
                Ok(service) => {
                    info!("LLMCompiler: Final response using Execution stage model: {}/{}", 
                        service.get_config().provider, service.get_config().model);
                    Some(service)
                }
                Err(e) => {
                    warn!("LLMCompiler: Failed to get Execution stage service: {}, using default", e);
                    self.ai_service.clone()
                }
            }
        } else {
            self.ai_service.clone()
        };

        if let Some(ai_service) = execution_service {
            info!(
                "å¼€å§‹è°ƒç”¨AIç”Ÿæˆæœ€ç»ˆå“åº”ï¼Œæç¤ºè¯é•¿åº¦: {} å­—ç¬¦",
                response_prompt.len()
            );
            debug!("AIå“åº”ç”Ÿæˆæç¤ºè¯: {}", response_prompt);

            // æ„å»ºæœ€ç»ˆå›ç­”çš„ user æç¤ºï¼ˆä»…åŒ…å«ä¸šåŠ¡ä¸Šä¸‹æ–‡ï¼Œä¸åŒ…å«ç³»ç»Ÿè¯´æ˜ï¼‰
            let outputs_block = successful_outputs
                .iter()
                .enumerate()
                .map(|(i, m)| {
                    let pretty = serde_json::to_string_pretty(m).unwrap_or_else(|_| "{}".to_string());
                    format!("Result {}:\n{}", i + 1, pretty)
                })
                .collect::<Vec<_>>()
                .join("\n\n");

            let final_user_prompt = format!(
                "Please generate a clear, friendly final answer for the user based on the results below.\n\nUser query:\n{}\n\nExecution summary:\n- total_tasks: {}\n- successful_tasks: {}\n- failed_tasks: {}\n- total_duration_ms: {}\n\nOutputs:\n{}\n",
                user_query,
                execution_summary.total_tasks,
                execution_summary.successful_tasks,
                execution_summary.failed_tasks,
                execution_summary.total_duration_ms,
                outputs_block
            );

            match ai_service
                .send_message_stream_with_save_control(
                    Some(&final_user_prompt),        // å‘é€ç»™LLMçš„ç”¨æˆ·æ¶ˆæ¯
                    None,                             // ä¸é‡å¤ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
                    Some(&response_prompt),           // ä½œä¸ºsystem promptä½¿ç”¨ï¼ˆæ¥è‡ªDBæ¨¡æ¿æˆ–é»˜è®¤ï¼‰
                    conversation_id.map(|s| s.to_string()),
                    message_id.map(|s| s.to_string()),
                    true,
                    false,
                    Some(ChunkType::Content),
                )
                .await
            {
                Ok(ai_response) => {
                    if ai_response.trim().is_empty() {
                        warn!("AIè¿”å›äº†ç©ºå“åº”ï¼Œä½¿ç”¨é»˜è®¤å“åº”");
                        Ok(self.generate_default_response(task_results, execution_summary))
                    } else {
                        info!("AIå“åº”ç”ŸæˆæˆåŠŸï¼Œå“åº”é•¿åº¦: {} å­—ç¬¦", ai_response.len());
                        Ok(ai_response)
                    }
                }
                Err(e) => {
                    error!("AIå“åº”ç”Ÿæˆå¤±è´¥: {}, ä½¿ç”¨é»˜è®¤å“åº”", e);
                    Ok(self.generate_default_response(task_results, execution_summary))
                }
            }
        } else {
            Ok(self.generate_default_response(task_results, execution_summary))
        }
    }

    /// æ‰§è¡ŒDAGä¸­çš„å•ä¸ªå·¥å…·ï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œï¼‰
    async fn execute_dag_tool_static(
        tool_system: &Arc<crate::tools::ToolSystem>,
        tool_name: &str,
        target: &str,
    ) -> Result<serde_json::Value> {
        // å‡†å¤‡å·¥å…·æ‰§è¡Œå‚æ•°
        let mut tool_inputs = HashMap::new();
        tool_inputs.insert("target".to_string(), serde_json::json!(target));

        // æ ¹æ®å·¥å…·ç±»å‹æ·»åŠ ç‰¹å®šå‚æ•°
        match tool_name {
            "port_scan" => {
                tool_inputs.insert("ports".to_string(), serde_json::json!("common"));
                tool_inputs.insert("threads".to_string(), serde_json::json!(100));
                // LLMCompilerä½¿ç”¨æ›´é«˜å¹¶å‘
            }
            "rsubdomain" => {
                tool_inputs.insert("wordlist".to_string(), serde_json::json!("common"));
            }
            _ => {}
        }

        let execution_params = ToolExecutionParams {
            inputs: tool_inputs,
            context: HashMap::new(),
            timeout: Some(std::time::Duration::from_secs(120)), // LLMCompilerä½¿ç”¨è¾ƒçŸ­è¶…æ—¶
            execution_id: Some(Uuid::new_v4()),
        };

        // æ‰§è¡Œå·¥å…·
        match tool_system.execute_tool(tool_name, execution_params).await {
            Ok(result) => Ok(if result.output.is_null() {
                serde_json::json!({"status": "completed"})
            } else {
                result.output
            }),
            Err(e) => Err(e),
        }
    }

    // å…¶ä»–è¾…åŠ©æ–¹æ³•
    fn build_default_response_prompt(
        &self,
        user_query: &str,
        successful_outputs: &[&std::collections::HashMap<String, serde_json::Value>],
        execution_summary: &ExecutionSummary,
    ) -> String {
        format!(
            r#"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®‰å…¨åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹LLMCompilerå·¥ä½œæµæ‰§è¡Œç»“æœï¼Œä¸ºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šã€‚

**ç”¨æˆ·æŸ¥è¯¢**: {}

**æ‰§è¡Œç»Ÿè®¡**:
- æ€»ä»»åŠ¡æ•°: {}
- æˆåŠŸä»»åŠ¡: {}
- å¤±è´¥ä»»åŠ¡: {}
- æ€»è€—æ—¶: {}ms
- ä»»åŠ¡æˆåŠŸç‡: {:.1}%

**æ‰§è¡Œç»“æœè¯¦æƒ…**:
{}

{}"#,
            user_query,
            execution_summary.total_tasks,
            execution_summary.successful_tasks,
            execution_summary.failed_tasks,
            execution_summary.total_duration_ms,
            if execution_summary.total_tasks > 0 {
                (execution_summary.successful_tasks as f32 / execution_summary.total_tasks as f32)
                    * 100.0
            } else {
                0.0
            },
            self.format_outputs_for_response(&successful_outputs),
            if execution_summary.failed_tasks > 0 {
                format!("\n## âŒ æ‰§è¡Œé—®é¢˜\næœ¬æ¬¡æ‰§è¡Œä¸­æœ‰ {} ä¸ªä»»åŠ¡å¤±è´¥ï¼Œè¯·åœ¨æŠ¥å‘Šä¸­è¯´æ˜å¯èƒ½çš„åŸå› å’Œå½±å“ã€‚", execution_summary.failed_tasks)
            } else {
                String::new()
            }
        )
    }

    fn format_outputs_for_response(
        &self,
        outputs: &[&std::collections::HashMap<String, serde_json::Value>],
    ) -> String {
        if outputs.is_empty() {
            return "æš‚æ— æˆåŠŸæ‰§è¡Œçš„ä»»åŠ¡è¾“å‡º".to_string();
        }

        outputs
            .iter()
            .enumerate()
            .map(|(i, output)| {
                let mut formatted_output = Vec::new();

                // æ ¼å¼åŒ–æ¯ä¸ªè¾“å‡ºå­—æ®µ
                for (key, value) in output.iter() {
                    let formatted_value = match value {
                        serde_json::Value::String(s) => s.clone(),
                        serde_json::Value::Number(n) => n.to_string(),
                        serde_json::Value::Bool(b) => b.to_string(),
                        serde_json::Value::Array(arr) => {
                            if arr.len() <= 5 {
                                format!(
                                    "[{}]",
                                    arr.iter()
                                        .map(|v| match v {
                                            serde_json::Value::String(s) => s.clone(),
                                            _ => v.to_string(),
                                        })
                                        .collect::<Vec<_>>()
                                        .join(", ")
                                )
                            } else {
                                format!("[{} ä¸ªé¡¹ç›®]", arr.len())
                            }
                        }
                        serde_json::Value::Object(_) => "[å¯¹è±¡æ•°æ®]".to_string(),
                        serde_json::Value::Null => "null".to_string(),
                    };

                    formatted_output.push(format!("  - {}: {}", key, formatted_value));
                }

                format!("### ä»»åŠ¡ {}\n{}", i + 1, formatted_output.join("\n"))
            })
            .collect::<Vec<_>>()
            .join("\n\n")
    }

    fn generate_default_response(
        &self,
        task_results: &[TaskExecutionResult],
        execution_summary: &ExecutionSummary,
    ) -> String {
        let successful_tasks = task_results
            .iter()
            .filter(|r| r.status == TaskStatus::Completed)
            .count();
        let failed_tasks = task_results
            .iter()
            .filter(|r| r.status == TaskStatus::Failed)
            .count();

        format!(
            "æ‰§è¡Œå®Œæˆï¼\n\n\
            ğŸ“Š æ‰§è¡Œç»Ÿè®¡:\n\
            - æ€»ä»»åŠ¡: {}\n\
            - æˆåŠŸä»»åŠ¡: {}\n\
            - å¤±è´¥ä»»åŠ¡: {}\n\
            - æ€»è€—æ—¶: {}ms\n\n\
            ğŸ“‹ ä¸»è¦ç»“æœ:\n\
            {}\n\n\
            {}",
            execution_summary.total_tasks,
            successful_tasks,
            failed_tasks,
            execution_summary.total_duration_ms,
            self.summarize_task_outputs(task_results),
            if failed_tasks > 0 {
                "âš ï¸ éƒ¨åˆ†ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥è¾“å…¥å‚æ•°æˆ–ç½‘ç»œè¿æ¥ã€‚"
            } else {
                "âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼"
            }
        )
    }

    fn summarize_task_outputs(&self, task_results: &[TaskExecutionResult]) -> String {
        let mut summaries = Vec::new();

        for result in task_results {
            if result.status == TaskStatus::Completed {
                let summary = if let Some(result_value) = result.outputs.get("result") {
                    format!("- {}: {}", result.task_id, result_value)
                } else {
                    format!("- {}: æ‰§è¡ŒæˆåŠŸ", result.task_id)
                };
                summaries.push(summary);
            } else if result.status == TaskStatus::Failed {
                let error_msg = result.error.as_deref().unwrap_or("æœªçŸ¥é”™è¯¯");
                summaries.push(format!("- {}: æ‰§è¡Œå¤±è´¥ - {}", result.task_id, error_msg));
            }
        }

        if summaries.is_empty() {
            "æ— å¯ç”¨ç»“æœ".to_string()
        } else {
            summaries.join("\n")
        }
    }

    fn calculate_efficiency_metrics(
        &self,
        execution_summary: &ExecutionSummary,
    ) -> EfficiencyMetrics {
        let total_tasks = execution_summary.total_tasks;

        let task_success_rate = if total_tasks > 0 {
            execution_summary.successful_tasks as f32 / total_tasks as f32
        } else {
            0.0
        };

        let average_task_duration_ms = if execution_summary.successful_tasks > 0 {
            execution_summary.total_duration_ms as f32 / execution_summary.successful_tasks as f32
        } else {
            0.0
        };

        // ç®€åŒ–çš„å¹¶è¡Œæ•ˆç‡è®¡ç®—ï¼ˆåŸºäºé‡è§„åˆ’æ¬¡æ•°+1ä½œä¸ºè½®æ¬¡ï¼‰
        let total_rounds = execution_summary.replanning_count + 1;
        let average_parallelism = if total_rounds > 0 {
            total_tasks as f32 / total_rounds as f32
        } else {
            0.0
        }
        .min(self.config.max_concurrency as f32);

        let resource_utilization = if self.config.max_concurrency > 0 {
            average_parallelism / self.config.max_concurrency as f32
        } else {
            0.0
        }
        .min(1.0);

        EfficiencyMetrics {
            average_parallelism,
            resource_utilization,
            task_success_rate,
            average_task_duration_ms,
        }
    }

    /// è·å–å¼•æ“çŠ¶æ€
    pub async fn get_engine_status(&self) -> EngineStatus {
        if let Some(task_fetcher) = &self.task_fetcher {
            let task_stats = task_fetcher.get_execution_stats().await;

            EngineStatus {
                is_running: task_stats.executing_tasks > 0,
                pending_tasks: task_stats.waiting_tasks + task_stats.ready_tasks,
                executing_tasks: task_stats.executing_tasks,
                completed_tasks: task_stats.completed_tasks,
                failed_tasks: task_stats.failed_tasks,
                available_capacity: self
                    .executor_pool
                    .as_ref()
                    .map_or(0, |e| e.available_permits()),
                total_capacity: self.config.max_concurrency,
            }
        } else {
            EngineStatus {
                is_running: false,
                pending_tasks: 0,
                executing_tasks: 0,
                completed_tasks: 0,
                failed_tasks: 0,
                available_capacity: self.config.max_concurrency,
                total_capacity: self.config.max_concurrency,
            }
        }
    }

    /// å–æ¶ˆå½“å‰æ‰§è¡Œ (å†…éƒ¨æ–¹æ³•)
    pub async fn cancel_current_execution(&self) -> Result<()> {
        info!("å–æ¶ˆå½“å‰æ‰§è¡Œ");
        if let Some(task_fetcher) = &self.task_fetcher {
            task_fetcher.cancel_pending_tasks().await?;
        }
        Ok(())
    }

    /// é‡ç½®å¼•æ“çŠ¶æ€
    pub async fn reset(&self) -> Result<()> {
        info!("é‡ç½®å¼•æ“çŠ¶æ€");
        if let Some(task_fetcher) = &self.task_fetcher {
            task_fetcher.cancel_pending_tasks().await?;
        }
        Ok(())
    }

    /// æ‰§è¡Œå·¥ä½œæµ - ä¸»å…¥å£ç‚¹ (ä»åŸæœ‰å¼•æ“ç§»æ¤)
    pub async fn execute_workflow(
        &self,
        user_query: &str,
        context: Option<std::collections::HashMap<String, serde_json::Value>>,
    ) -> Result<WorkflowExecutionResult> {
        // è¿™ä¸ªæ–¹æ³•ä¿æŒä¸åŸæœ‰å¼•æ“å®Œå…¨ä¸€è‡´çš„é€»è¾‘
        // info!("å¼€å§‹æ‰§è¡ŒLLMCompilerå·¥ä½œæµ: {}", user_query);

        let context = context.unwrap_or_default();
        let mut execution_summary = ExecutionSummary {
            total_tasks: 0,
            successful_tasks: 0,
            failed_tasks: 0,
            total_duration_ms: 0,
            replanning_count: 0,
            key_findings: Vec::new(),
            efficiency_metrics: EfficiencyMetrics {
                average_parallelism: 0.0,
                resource_utilization: 0.0,
                task_success_rate: 0.0,
                average_task_duration_ms: 0.0,
            },
        };

        let workflow_start = std::time::Instant::now();
        let mut current_plan: Option<DagExecutionPlan> = None;
        let mut all_results: Vec<TaskExecutionResult> = Vec::new();

        // ä¸»æ‰§è¡Œå¾ªç¯
        for round in 1..=self.config.max_iterations {
            info!("å¼€å§‹æ‰§è¡Œè½®æ¬¡: {}/{}", round, self.config.max_iterations);
            // æ›´æ–°æ€»è½®æ¬¡ï¼ˆè¿™é‡Œç”¨replanning_countæ¥è·Ÿè¸ªï¼‰
            if round > 1 {
                execution_summary.replanning_count = round - 1;
            }

            // 1. è§„åˆ’é˜¶æ®µ
            let execution_plan = if let Some(ref plan) = current_plan {
                // å¦‚æœæœ‰ç°æœ‰è®¡åˆ’ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’
                if self.config.enable_replanning && round > 1 {
                    info!("æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’...");
                    // è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ–°è§„åˆ’çš„é€»è¾‘
                    plan.clone()
                } else {
                    plan.clone()
                }
            } else {
                // ç”Ÿæˆåˆå§‹è®¡åˆ’
                info!("ç”Ÿæˆåˆå§‹æ‰§è¡Œè®¡åˆ’...");

                if let Some(planner) = &self.planner {
                    let plan = planner.generate_dag_plan(user_query, &context).await?;
                    current_plan = Some(plan.clone());
                    plan
                } else {
                    return Err(anyhow::anyhow!("Planner not available"));
                }
            };

            info!("æ‰§è¡Œè®¡åˆ’åŒ…å« {} ä¸ªä»»åŠ¡", execution_plan.nodes.len());

            // 2. ä»»åŠ¡è°ƒåº¦é˜¶æ®µ
            if let Some(task_fetcher) = &self.task_fetcher {
                task_fetcher.initialize_plan(&execution_plan).await?;

                // 3. å¹¶è¡Œæ‰§è¡Œé˜¶æ®µ
                let round_results = self.execute_parallel_round().await?;

                if round_results.is_empty() {
                    warn!("è½®æ¬¡ {} æ²¡æœ‰æ‰§è¡Œä»»ä½•ä»»åŠ¡", round);
                    break;
                }

                // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                let completed_count = round_results
                    .iter()
                    .filter(|r| r.status == TaskStatus::Completed)
                    .count();
                let failed_count = round_results
                    .iter()
                    .filter(|r| r.status == TaskStatus::Failed)
                    .count();

                execution_summary.successful_tasks += completed_count;
                execution_summary.failed_tasks += failed_count;
                execution_summary.total_tasks += round_results.len();

                let round_duration: u64 = round_results.iter().map(|r| r.duration_ms).sum();
                execution_summary.total_duration_ms += round_duration;

                all_results.extend(round_results.clone());

                info!(
                    "è½®æ¬¡ {} å®Œæˆ: æˆåŠŸ {} ä¸ªä»»åŠ¡, å¤±è´¥ {} ä¸ªä»»åŠ¡, è€—æ—¶ {}ms",
                    round, completed_count, failed_count, round_duration
                );

                // 4. æ™ºèƒ½å†³ç­–é˜¶æ®µ
                if let Some(joiner) = &self.joiner {
                    let mut joiner_guard = joiner.lock().await;
                    let decision = joiner_guard
                        .analyze_and_decide(user_query, &execution_plan, &round_results, round)
                        .await?;

                    // è®°å½•å†³ç­–ä¿¡æ¯ï¼ˆå¯ä»¥æ·»åŠ åˆ°key_findingsä¸­ï¼‰
                    match &decision {
                        JoinerDecision::Complete { response, .. } => {
                            execution_summary
                                .key_findings
                                .push(format!("å†³ç­–: å®Œæˆæ‰§è¡Œ - {}", response));
                        }
                        JoinerDecision::Continue { feedback, .. } => {
                            execution_summary
                                .key_findings
                                .push(format!("å†³ç­–: ç»§ç»­æ‰§è¡Œ - {}", feedback));
                        }
                    }

                    match decision {
                        JoinerDecision::Complete { response, .. } => {
                            info!("Joiner decided to complete: {}", response);
                            break;
                        }
                        JoinerDecision::Continue {
                            feedback,
                            suggested_tasks,
                            ..
                        } => {
                            info!("Joiner decided to continue: {}", feedback);

                            // å¦‚æœæœ‰å»ºè®®çš„æ–°ä»»åŠ¡ï¼Œè§¦å‘é‡è§„åˆ’
                            if !suggested_tasks.is_empty() {
                                info!(
                                    "Adding {} suggested tasks from Joiner",
                                    suggested_tasks.len()
                                );
                                for new_task in suggested_tasks {
                                    // é€šè¿‡äº‹ä»¶ç³»ç»Ÿæ·»åŠ æ–°ä»»åŠ¡
                                    if let Err(e) = task_fetcher.send_event(
                                        crate::engines::llm_compiler::task_fetcher::SchedulingEvent::TaskAdded {
                                            task: new_task
                                        }
                                    ) {
                                        warn!("Failed to add suggested task: {}", e);
                                    }
                                }
                                execution_summary.replanning_count += 1;
                            } else {
                                // æ²¡æœ‰æ–°ä»»åŠ¡ä½†å†³å®šç»§ç»­ï¼Œè§¦å‘é‡è§„åˆ’
                                if self.config.enable_replanning
                                    && round < self.config.max_iterations
                                {
                                    info!("Triggering replanning due to Continue decision");
                                    match self
                                        .trigger_replanning(
                                            &execution_plan,
                                            &round_results,
                                            &feedback,
                                            task_fetcher,
                                        )
                                        .await
                                    {
                                        Ok(new_tasks_added) => {
                                            if new_tasks_added > 0 {
                                                execution_summary.replanning_count += 1;
                                                info!(
                                                    "Replanning added {} new tasks",
                                                    new_tasks_added
                                                );
                                            } else {
                                                info!("Replanning did not generate new tasks");
                                            }
                                        }
                                        Err(e) => {
                                            warn!("Replanning failed: {}", e);
                                        }
                                    }
                                }
                            }
                        }
                    }
                }

                // æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡
                if !task_fetcher.has_pending_tasks().await {
                    info!("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œç»“æŸæ‰§è¡Œ");
                    break;
                }
            }
        }

        let total_duration = workflow_start.elapsed();
        execution_summary.total_duration_ms = total_duration.as_millis() as u64;

        // è®¡ç®—æœ€ç»ˆæ•ˆç‡æŒ‡æ ‡
        execution_summary.efficiency_metrics =
            self.calculate_efficiency_metrics(&execution_summary);

        info!(
            "å·¥ä½œæµæ‰§è¡Œå®Œæˆ: {} ä¸ªä»»åŠ¡, æˆåŠŸ {}, å¤±è´¥ {}, è€—æ—¶ {}ms",
            execution_summary.total_tasks,
            execution_summary.successful_tasks,
            execution_summary.failed_tasks,
            execution_summary.total_duration_ms
        );

        // ç”Ÿæˆæœ€ç»ˆç»“æœï¼ˆæš‚ä¸ä¼ é€’IDï¼Œå› ä¸ºè¿™ä¸ªæ–¹æ³•å¯èƒ½æ˜¯æµ‹è¯•/å•ç‹¬ä½¿ç”¨ï¼‰
        let final_response = self
            .generate_final_response(user_query, &all_results, &execution_summary, None, None)
            .await?;

        Ok(WorkflowExecutionResult {
            success: true,
            response: final_response,
            execution_summary: execution_summary.clone(),
            task_results: all_results,
            efficiency_metrics: execution_summary.efficiency_metrics,
            error: None,
        })
    }

    /// è§¦å‘é‡è§„åˆ’å¹¶æ·»åŠ æ–°ä»»åŠ¡
    async fn trigger_replanning(
        &self,
        original_plan: &DagExecutionPlan,
        execution_results: &[TaskExecutionResult],
        feedback: &str,
        task_fetcher: &Arc<TaskFetchingUnit>,
    ) -> Result<usize> {
        info!("Starting replanning process");

        if let Some(planner) = &self.planner {
            // è°ƒç”¨è§„åˆ’å™¨è¿›è¡Œé‡è§„åˆ’
            let new_plan = planner
                .replan(original_plan, execution_results, feedback)
                .await?;

            let new_tasks_count = new_plan.nodes.len();
            info!("Replanning generated {} new tasks", new_tasks_count);

            // å°†æ–°ä»»åŠ¡æ·»åŠ åˆ°è°ƒåº¦å™¨
            for new_task in new_plan.nodes {
                task_fetcher.send_event(
                    crate::engines::llm_compiler::task_fetcher::SchedulingEvent::TaskAdded {
                        task: new_task,
                    },
                )?;
            }

            // æ›´æ–°ä¾èµ–å›¾ï¼ˆå¦‚æœæœ‰æ–°çš„ä¾èµ–å…³ç³»ï¼‰
            if !new_plan.dependency_graph.is_empty() {
                task_fetcher
                    .merge_dependency_graph(&new_plan.dependency_graph)
                    .await?;
            }

            // æ›´æ–°å˜é‡æ˜ å°„
            if !new_plan.variable_mappings.is_empty() {
                task_fetcher
                    .merge_variable_mappings(&new_plan.variable_mappings)
                    .await?;
            }

            Ok(new_tasks_count)
        } else {
            Err(anyhow::anyhow!("Planner not available for replanning"))
        }
    }
}

/// å·¥ä½œæµæ‰§è¡Œç»“æœ
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct WorkflowExecutionResult {
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// æœ€ç»ˆå“åº”
    pub response: String,
    /// æ‰§è¡Œæ‘˜è¦
    pub execution_summary: ExecutionSummary,
    /// ä»»åŠ¡ç»“æœåˆ—è¡¨
    pub task_results: Vec<TaskExecutionResult>,
    /// æ•ˆç‡æŒ‡æ ‡
    pub efficiency_metrics: EfficiencyMetrics,
    /// é”™è¯¯ä¿¡æ¯
    pub error: Option<String>,
}

/// å¼•æ“çŠ¶æ€
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct EngineStatus {
    /// æ˜¯å¦æ­£åœ¨è¿è¡Œ
    pub is_running: bool,
    /// å¾…æ‰§è¡Œä»»åŠ¡æ•°
    pub pending_tasks: usize,
    /// æ‰§è¡Œä¸­ä»»åŠ¡æ•°
    pub executing_tasks: usize,
    /// å·²å®Œæˆä»»åŠ¡æ•°
    pub completed_tasks: usize,
    /// å¤±è´¥ä»»åŠ¡æ•°
    pub failed_tasks: usize,
    /// å¯ç”¨æ‰§è¡Œå®¹é‡
    pub available_capacity: usize,
    /// æ€»æ‰§è¡Œå®¹é‡
    pub total_capacity: usize,
}
